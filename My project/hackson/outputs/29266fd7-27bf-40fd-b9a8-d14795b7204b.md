# Qwen3-VL Technical Report


2025年12月1日
Qwen3-VL 技术报告
Qwen 团队
https://chat.qwen.ai
https://huggingface.co/Qwen
https://modelscope.cn/organization/qwen
https://github.com/QwenLM/Qwen3-VL
摘要
我们介绍了 Qwen3-VL，这是迄今为止 Qwen 系列中最强大的视觉-语言模型，在广泛的多模态基准测试中表现出色。该模型本机支持最多 256K 个标记的交错上下文，无缝集成文本、图像和视频。模型家族包括密集型（2B/4B/8B/32B）和专家混合型（30B-A3B/235B-A22B）变体，以适应不同的延迟-质量权衡。Qwen3-VL 提供了三个核心支柱：(i) 显著更强的纯文本理解能力，在某些情况下超过了类似的纯文本骨干模型；(ii) 基于本机 256K 标记窗口的强大长上下文理解能力，适用于文本和交错多模态输入，能够忠实地保留、检索和跨长文档和视频进行交叉引用；(iii) 在单图像、多图像和视频任务中表现出领先的多模态推理能力，在 MMMU 和视觉数学基准测试（例如 Math-Vista 和 MathVision）中表现出色。在架构上，我们引入了三项关键升级：(i) 增强的交错-MRoPE，用于加强图像和视频的空间-时间建模；(ii) DeepStack 集成，有效利用多级 ViT 特征，加强视觉-语言对齐；(iii) 视频的时间对齐，从 T-RoPE 演变为显式的文本时间戳对齐，实现更精确的时间定位。为了平衡纯文本和多模态学习目标，我们应用了平方根重新加权，这在不牺牲文本能力的情况下提升了多模态性能。
我们将预训练扩展到 256K 标记的上下文长度，并将后训练分为非思考和思考变体，以满足不同的应用需求。此外，我们在后训练阶段分配了额外的计算资源，以进一步提升模型性能。在相同的标记预算和延迟约束下，Qwen3-VL 在密集型和专家混合型（MoE）架构中均表现出色。我们设想 Qwen3-VL 将作为图像基础推理、代理决策和多模态代码智能在实际工作流程中的基础引擎。
1arXiv:2511.21631v2  [cs.CV]  27 Nov 2025

1 引言
视觉-语言模型（VLMs）近年来取得了实质性进展，从基础的视觉感知发展到图像和视频之间的高级多模态推理。VLMs 的快速发展催生了下游应用的迅速扩展——例如长上下文理解、STEM 推理、GUI 理解与交互以及代理工作流。至关重要的是，这些进步不应侵蚀底层大语言模型（LLMs）的语言能力；多模态模型在语言基准测试中应达到或超过其纯文本对应模型的表现。
在本报告中，我们介绍了 Qwen3-VL 及其在通用和高级应用方面的进展。基于 Qwen3 系列（Yang et al., 2025a），我们实例化了四个密集模型（2B/4B/8B/32B）和两个混合专家（MoE）模型（30B-A3B / 235B-A22B），每个模型都以最多 256K 个标记的上下文窗口进行训练，以实现长上下文理解。通过优化训练语料库和训练策略，我们在视觉-语言（VL）训练过程中保留了底层 LLM 的语言能力，从而显著提高了整体能力。我们发布了非思考版和思考版两种变体；后者展示了显著更强的多模态推理能力，在复杂推理任务中表现出色。
首先介绍架构改进，这些改进涉及三个组件：1) 增强的位置编码。在 Qwen2.5-VL 中，我们使用 MRoPE 作为文本和视觉的统一位置编码方案。我们观察到，将嵌入维度切分为时间（t）、水平（h）和垂直（w）组会导致频率谱不平衡，阻碍长视频理解。因此，我们采用了交错的 MRoPE，将 t、h 和 w 均匀分布在低频和高频带之间，从而生成更忠实的位置表示。2) 深层堆叠用于跨层融合。为了加强视觉-语言对齐，我们引入了开创性的 DeepStack（Meng et al., 2024）机制。来自视觉编码器不同层的视觉标记通过轻量级残差连接路由到相应的 LLM 层，增强多层级融合而不增加额外的上下文长度。3) 显式视频时间戳。我们将 Qwen2.5-VL 中通过位置编码实现的绝对时间对齐替换为显式时间戳标记，以标记帧组，提供更简单直接的时间表示。此外，在优化方面，我们将每样本损失改为每标记平方根归一化损失，更好地平衡了训练过程中文本和多模态数据的贡献。
为了构建一个更强大和稳健的视觉-语言基础模型，我们从质量、多样性和结构方面全面改革了训练数据。关键升级包括增强的字幕监督、扩展的全识别和 OCR 覆盖范围、标准化的 3D/空间推理定位，以及新的代码、长文档和时间定位视频语料库。我们进一步注入了链式思维推理和高质量、多样化的 GUI 代理交互数据，以连接感知、推理和行动。这些创新共同实现了更强的多模态理解、精确的定位和工具增强的智能。
我们的训练管道包括两个阶段：预训练和后训练。预训练分为四个阶段：首先是仅更新合并层（视觉-语言投影层）而冻结其余模型的对齐热身阶段，随后是逐步增加上下文窗口大小的全参数训练，分别在 8K、32K 和 256K 序列长度下进行。后训练包括三个阶段：(i) 在长链式思维数据上进行监督微调，(ii) 从更强的教师模型中进行知识蒸馏，(iii) 强化学习。
上述创新使 Qwen3-VL 不仅作为一个稳健的视觉-语言基础模型具备强大的能力，而且作为一个灵活的平台，能够实现现实世界中的多模态智能——无缝集成感知、推理和行动，跨越各种应用领域。在接下来的部分中，我们将介绍模型架构、训练框架以及广泛的评估，展示其在文本、视觉和多模态推理基准测试中的一致性和竞争力。
2 模型架构
继 Qwen2.5-VL（Bai et al., 2025）之后，Qwen3-VL 采用了一个由视觉编码器、基于 MLP 的视觉-语言合并器和大语言模型（LLM）组成的三模块架构。图 1 描述了详细的模型结构。
大语言模型：Qwen3-VL 实例化为三种密集变体（Qwen3-VL-2B/4B/8B/32B）和两种 MoE 变体（Qwen3-VL-30B-A3B，Qwen3-VL-235B-A22B），均基于 Qwen3 骨干。旗舰模型 Qwen3-VL-235B-A22B 总参数量为 235B，每标记激活 22B 参数。它

图1：Qwen3-VL框架集成了视觉编码器和语言模型解码器，以处理多模态输入，包括文本、图像和视频。视觉编码器专门设计用于处理动态的、原生分辨率的视觉输入，并将其映射为可变长度的视觉标记。为了增强感知能力和保留丰富的视觉信息，我们引入了开创性的DeepStack机制，该机制从视觉编码器的多个层中注入视觉标记到LLM的相应层。此外，我们采用交错MRoPE来编码多模态输入的位置信息，使其具有平衡的频率谱，并引入基于文本的时间戳标记，以更有效地捕捉视频序列的时间结构。
Qwen3-VL在广泛的多模态任务中优于大多数VLM，并在大多数语言基准测试中超越其仅文本的对应模型。

视觉编码器：我们使用SigLIP-2架构（Tschannen等，2025）作为视觉编码器，并继续使用动态输入分辨率进行训练，初始化自官方预训练检查点。为了有效适应动态分辨率，我们采用2D-RoPE并根据输入大小插值绝对位置嵌入，遵循CoMP（Chen等，2025）的方法。具体而言，我们默认使用SigLIP2-SO-400M变体，并为小规模LLM（2B和4B）使用SigLIP2-Large（300M）。

基于MLP的视觉-语言融合器：与Qwen2.5-VL类似，我们使用两层MLP将视觉编码器的2×2视觉特征压缩为一个视觉标记，与LLM的隐藏维度对齐。此外，我们部署了专门的融合器以支持DeepStack机制（Meng等，2024），详细内容见第2.2节。

2.1 交错MRoPE
Qwen2-VL（Wang等，2024c）引入了MRoPE来建模多模态输入的位置信息。在其原始形式中，嵌入维度被划分为时间（t）、水平（h）和垂直（w）子空间，每个子空间分配不同的旋转频率。这导致了不平衡的频率谱，后续研究表明这会降低长视频理解基准测试的性能。为了解决这一问题，我们重新设计了频率分配，通过在嵌入维度上交错t、h和w组件（Huang等，2025）。这确保了每个时空轴在低频和高频带中均匀表示。结果是平衡的频率谱减轻了原始的频谱偏差，并显著提高了视频的长距离位置建模能力。

# 2.2 DeepStack
我们从 DeepStack (Meng et al., 2024) 中汲取灵感，并将视觉标记注入到语言模型的多个层中。与原始的 DeepStack 方法不同，后者堆叠来自多尺度视觉输入的标记，我们将 DeepStack 扩展到从 Vision Transformer (ViT) 的中间层提取视觉标记。这种设计保留了丰富的视觉信息，涵盖了从低级到高级的表示。具体来说，如图 1 所示，我们从视觉编码器的三个不同层级选择特征。随后，专门的视觉-语言融合模块将这些多层次特征投影为视觉标记，并直接添加到前三个语言模型层的相应隐藏状态中。

2.3 视频时间戳
在 Qwen2.5-VL 中，采用了一种时间同步的 MRoPE 变体，以赋予模型时间感知能力。然而，我们发现了这种方法的两个关键限制：(1) 通过将时间位置 ID 直接绑定到绝对时间，该方法为长视频生成了过大且稀疏的时间位置 ID，降低了模型理解长时间上下文的能力。(2) 在这种方案下有效学习需要在各种帧率 (fps) 下进行广泛且均匀分布的采样，显著增加了训练数据构建的成本。
为了解决这些问题，我们采用了基于文本标记的时间编码策略 (Chen et al., 2024b)，其中每个视频时间片段都以前缀为格式化文本字符串的时间戳开头——例如，<3.0 秒>。此外，在训练过程中，我们生成秒和 HMS (小时:分钟:秒) 格式的时间戳，以确保模型学会解释不同的时间码表示。虽然这种方法会适度增加上下文长度，但它使模型能够更有效地感知时间信息，从而促进时间感知的视频任务，如视频定位和密集字幕。

3 预训练
3.1 训练配方
我们首先通过基于预训练的 SigLIP-2 模型进行动态分辨率的连续训练来增强视觉编码器。整体 Qwen3-VL 模型采用三模块架构，包括此视觉编码器、基于 MLP 的视觉-语言融合模块以及 Qwen3 大语言模型 (LLM) 主干。在此架构基础上，我们的预训练方法系统地分为四个不同的阶段，旨在逐步建立从基本对齐到长上下文理解的能力。这些阶段的概述见表 1。
表 1: Qwen3-VL 在不同阶段的训练设置和超参数。
阶段 目标 训练令牌预算 序列长度
S0 视觉-语言对齐融合 670 亿 8,192
S1 多模态预训练 全部 约 1 万亿 8,192
S2 长上下文预训练 全部 约 1 万亿 32,768
S3 超长上下文适应 全部 1000 亿 262,144
阶段 0：视觉-语言对齐。初始阶段 (S0) 专注于高效弥合视觉编码器和语言模型之间的模态差距。至关重要的是，仅在这一阶段训练 MLP 融合模块的参数，而视觉编码器和语言模型主干保持冻结。我们使用了一个精心策划的数据集，包含约 670 亿个令牌，由高质量的图像-字幕对、视觉知识集合和光学字符识别 (OCR) 数据组成。所有训练均在序列长度为 8,192 的情况下进行。这种先对齐的方法为跨模态理解奠定了坚实的基础，然后再进行全参数训练。
阶段 1：多模态预训练。在初始对齐之后，阶段 1 (S1) 过渡到全参数多模态预训练。在这个阶段，我们解冻所有模型组件——视觉编码器、融合模块和语言模型——进行联合端到端训练。模型在一个庞大且多样化的数据集上进行训练，该数据集包含约 1 万亿 (1T) 个令牌。为了保持语言模型强大的语言能力，数据混合由视觉-语言 (VL) 数据和纯文本数据组成。VL 部分丰富多样，增加了交错的图像-文本文档、视觉定位任务和视觉问题。

视觉问答（Visual Question Answering, VQA）、STEM领域的数据以及少量视频数据，以引入时间理解。序列长度保持在8,192。
第二阶段：长上下文预训练。第二阶段（S2）旨在显著扩展模型的上下文处理能力。这一阶段的关键变化是将序列长度增加四倍至32,768，同时所有模型参数继续可训练。训练在大约1T标记的数据集上进行，数据混合进行了调整以支持长上下文任务。文本数据的比例增加，以增强长篇文本的理解能力，而剩余的视觉语言（VL）数据则包含大量视频和面向代理的指令跟随数据。这一阶段对于使模型能够处理和推理更长的视频和复杂的多步骤任务至关重要。
第三阶段：超长上下文适应。最终阶段（S3）是一个专门的阶段，旨在将模型的上下文窗口推向其操作极限。在此阶段，我们将序列长度大幅增加到262,144。模型在一个专门为该目的策划的100B标记数据集上进行训练。数据由纯文本数据和视觉语言（VL）数据组成，重点放在长视频和长文档理解任务上。这一最终适应巩固了Qwen3-VL在处理和分析极长顺序输入方面的能力，这是全面文档分析和长时间视频摘要等应用的关键能力。
3.2 预训练数据
3.2.1 图像字幕和交错文本图像数据
为了构建一个通用的视觉-语言理解基础模型，我们显著扩展和优化了两个核心数据模态：图像-字幕对和交错的文本-图像序列。我们的策略强调高质量、多样性和语义丰富的多模态基础，这得到了定制模型和严格的过滤管道的支持。
图像字幕数据：我们从网络资源中整理了一个大规模的当代、主要为中英文双语的图像-文本对语料库，并应用了一个以专门的Qwen2.5-VL-32B模型微调的多阶段精炼管道，用于重新生成字幕。该模型利用每张图像的原始文本生成更全面、流畅和细致的字幕——丰富了视觉元素（例如，对象属性、空间布局和上下文语义）的描述，同时提高了文本部分的语言质量和信息性。
去重仅在重新生成的字幕文本上使用语义相似度指标进行，确保移除冗余样本而不牺牲视觉多样性。为了进一步提高未充分代表概念的覆盖率，我们在视觉嵌入上应用聚类（Johnson et al., 2019; Douze et al., 2024; Diao et al., 2025），以识别数据分布中的稀疏区域并进行有针对性的增强。结果是一个高保真的字幕数据集，平衡了规模、多样性和描述的细致程度。
交错文本图像数据：我们从最近的中文和英文网站（Laurençon et al., 2023; Zhu et al., 2023; Li et al., 2024c）收集了多样化的现实世界多模态文档。所有文档都通过一个轻量级的Qwen基评分器进行领域分类，该评分器经过微调以进行细粒度的领域识别。基于跨领域的验证实验，我们系统地排除了有害或低价值类别——如广告、促销内容和点击诱饵——使用相同的高效评分器过滤掉不希望的样本。
对于书籍规模的交错数据，我们使用微调的Qwen2.5-VL-7B模型进行高精度的多模态解析，精确提取并对齐嵌入的图表、图示和照片。为了实现超长上下文建模，我们通过合并连续页面构建了一个专门的子集，形成最多256K标记的序列，保持自然的页面顺序和多模态连贯性。在预处理过程中，我们实施严格的质量控制：(i) 纯文本或低对齐度的段落被移除；(ii) 对于超长书籍序列，我们要求最低页数和最低图像与文本比例，以确保在整个上下文中都有有意义的视觉-文本交互。这产生了一个干净、多样化且布局感知的交错语料库，既适用于基础理解，也适用于长距离多模态推理。
3.2.2 知识
世界知识对于多模态大语言模型（Multimodal Large Language Models, MLLMs）实现稳健的视觉理解、基础推理和实体感知生成在各种下游任务中至关重要。为了使Qwen3-VL具备对现实世界和虚构概念的全面理解，我们构建了一个5

大规模预训练数据集以明确定义的实体为中心，涵盖超过十几个语义类别——包括动物、植物、地标、食物以及日常物品，如车辆、电子产品和服装。
现实世界中的实体遵循长尾分布：突出的概念频繁出现且具有高质量标注，而大多数实体则较为罕见。为了解决这种不平衡，我们采用了一种基于重要性的采样策略。高知名度实体被更频繁地采样，以确保足够的学习信号，而低知名度实体则以较小的比例包含在内，以保持广泛的覆盖范围而不压垮训练过程。这种方法有效地平衡了数据质量、实用性和多样性。
所有保留的样本都经过多阶段精炼流程。除了标准的噪声和错位过滤外，我们还用更丰富的、大型语言模型生成的描述替换原始或稀疏的标题——例如通用的替代文本。这些增强的标题不仅识别主要实体，还描述其视觉属性、周围环境、空间布局以及与其他对象或人的互动，从而提供更完整和有根据的文本表示。
这些努力共同产生了一个知识丰富、上下文感知且注重区分的训练信号，显著增强了Qwen3-VL识别、推理和准确描述现实场景中视觉概念的能力。

3.2.3 OCR、文档解析和长文档理解
OCR：为了提高在现实世界图像上的OCR性能，我们使用粗到细的流水线收集了3000万个内部采集的样本。该流水线通过整合OCR专业模型的伪标签和Qwen2.5-VL的改进——无需人工标注——来细化OCR注释。除了Qwen2.5-VL支持的10种语言（不包括中文和英语）之外，我们还纳入了另外29种语言，合成了约3000万个高质量的多语言OCR样本，并整理了100多万张内部现实世界的多语言图像。
文档解析：对于文档解析，我们从Common Crawl收集了300万份PDF，均匀分布在10种文档类型中（每种类型30万份样本），以及400万份内部文档。内部布局模型首先预测文本和非文本区域的阅读顺序和边界框；然后，Qwen2.5-VL-72B执行区域特定的识别。输出被重新组装成位置感知、布局对齐的解析数据。
为了确保在异构格式下的稳健解析，我们设计了一个支持两种表示的统一注释框架：
• QwenVL-HTML，包括细粒度的元素级边界框；
• QwenVL-Markdown，其中仅定位图像和表格，表格以LaTeX编码。
我们构建了一个具有精确注释的大规模合成HTML语料库，并系统地将其转换为Markdown格式。为了进一步提高模型的泛化能力，我们在大量真实文档上生成伪标签并进行质量过滤。最终的训练集结合了合成数据和高质量的伪标签数据，以增强可扩展性和鲁棒性。
长文档理解：为了增强模型理解多页PDF的能力——通常跨越数十页——我们利用了一个大规模的长文档数据语料库。首先，我们通过合并单页文档样本合成长文档解析序列。在每个序列中，多个页面图像位于开头，随后是来自OCR或HTML解析的相应文本。其次，我们构建了长文档视觉问答（VQA）数据。具体来说，我们采样高质量的多页PDF，并生成多样化的VQA示例，要求模型跨多个页面和异构文档元素（如图表、表格、图形和正文文本）进行推理。我们仔细平衡了问题类型的分布，并确保支持证据来自广泛模态和布局组件，从而促进在扩展上下文中进行稳健、有根据和多跳推理。

3.2.4 定位和计数
视觉定位是多模态模型的基本能力，使它们能够准确识别、解释和定位从特定对象到任意图像区域的广泛视觉目标。在Qwen3-VL中，我们系统地增强了定位能力，并支持两种定位模式：边界框和点。这些表示允许在不同场景和下游任务中对图像内容进行精确和灵活的解释。此外，我们扩展了定位能力，包括：

该模型支持计数，从而实现对视觉实体的定量推理。接下来，我们简要描述用于定位和计数的数据构建管道。

基于框的定位（Box-based Grounding）：我们首先聚合了广泛使用的开源数据集，包括 COCO (Lin et al., 2014)、Objects365 (Shao et al., 2019)、OpenImages (Kuznetsova et al., 2020) 和 RefCOCO/+/g (Kazemzadeh et al., 2014; Mao et al., 2016)。为了进一步丰富数据多样性，我们开发了一条自动化合成管道，该管道能够在广泛的场景中生成高质量的对象标注。该管道分为三个阶段：(i) 使用 Qwen2.5-VL 从未标注的图像中提取对象候选；(ii) 使用开放词汇检测器（特别是 Grounding DINO (Liu et al., 2023a)）和 Qwen2.5-VL 对这些候选进行定位和标注；(iii) 对生成的标注进行质量评估，并系统地过滤掉低置信度或不准确的标注。通过这种方法，我们构建了一个大规模且高度多样化的基于框的定位数据集，涵盖了广泛的视觉上下文和对象类别。

基于点的定位（Point-based Grounding）：为了确保稳健的基于点的定位，我们整理了一个综合数据集，结合了公开可用和合成生成的指向标注。它整合了三个来源：(i) 来自 PixMo (Deitke et al., 2024) 的公共指向和计数标注；(ii) 从公共目标检测和实例分割基准派生的对象定位数据；(iii) 由专门设计的合成管道生成的高精度指向标注，该管道旨在针对细粒度的图像细节。

计数（Counting）：在定位数据的基础上，我们整理了一个高质量的子集，形成了我们的计数数据集，该数据集包括三种不同的任务形式：直接计数、基于框的计数和基于点的计数。这三种任务类型共同构成了一个全面的计数数据集。

与 Qwen2.5-VL 不同，我们在这一版本中采用了归一化的坐标系统，范围为 [0, 1000]。这种设计提高了对不同输入分辨率和纵横比变化的鲁棒性，同时简化了后处理并增强了预测坐标的下游应用中的可用性。

3.2.5 空间理解和三维识别
为了促进与物理世界的复杂互动，Qwen3-VL 被设计为具有深刻的空间理解能力。这使得模型能够解释空间关系、推断对象的功能，并执行动作规划和具身推理。它还可以从单个单目图像估计对象的三维空间位置。为了支持这些能力，我们创建了两个专注于空间理解和三维定位的综合数据集。

空间理解（Spatial Understanding）：除了定位对象外，Qwen3-VL 还被训练以推理二维场景中的空间关系、对象功能和可行的动作——这些能力对于具身人工智能和交互式应用至关重要。为此，我们构建了一个专门的数据集，该数据集不仅包括标准的定位，还包含：(i) 关系标注（例如，“笔记本电脑左侧的杯子”），(ii) 功能标签（例如，“可抓取”，“可按压”，“可坐”），以及 (iii) 需要规划的动作条件查询（例如，“我应该先移动什么才能拿到显示器后面的书？”）。这些样本来自精心策划的真实世界场景和合成生成的布局，自然语言查询通过模板化和大语言模型方法自动生成，以确保多样性和复杂性。关键在于，所有空间引用都是相对于其他对象或场景框架表达的，而不是绝对坐标，这鼓励了稳健的关系推理。这种训练使 Qwen3-VL 不仅能回答“在哪里”的问题，还能回答“如何”和“可以做什么”——为与视觉环境的代理互动奠定了基础。

三维定位（3D Grounding）：为了进一步增强模型从图像中理解物理世界的能力，我们构建了一个专门的预训练数据集，用于三维视觉定位。我们从公共的多样化室内和室外场景集合中获取数据，并将其重新表述为视觉问答格式。每个样本包括：1) 单视图相机图像，2) 自然语言指称表达，以及 3) 相应的 9 自由度 3D 边界框标注，以结构化的 JSON 格式指定对象的空间位置和语义标签。由于 3D 边界框是从多个传感器和数据源派生的，它们表现出不同的相机内参和固有噪声。为此，我们过滤掉了严重遮挡和不准确的标签，并遵循 Omni3D (Brazil et al., 2023) 将所有数据统一到虚拟相机坐标系中。我们还合成了大量描述性字幕，以创建丰富的文本查询用于三维定位。这些描述不仅包括对象类别的名称，还包括详细的属性、布局安排、空间位置、视觉功能和与周围对象的互动——从而生成更细粒度和接地的指称表达。

## 3.2.6 代码
我们通过将两类与代码相关的数据纳入训练语料库，增强了 Qwen3-VL 系列的专用编码能力，使模型能够在纯文本和视觉支持的上下文中阅读、编写和推理程序。
纯文本编码。我们重用了来自 Qwen3 和 Qwen3-Coder 系列的大量代码语料库。这个大规模数据集涵盖了广泛的编程语言和领域——包括软件开发、算法问题解决、数学推理和面向代理的任务——建立了模型对代码语法、算法逻辑和通用程序生成的基础理解。
多模态编码。为了应对需要视觉理解和代码生成的任务，我们策划了多种多模态编码任务的数据。这些数据来自开源数据集和内部合成管道，教会模型同时理解视觉输入并生成功能代码。数据涵盖了几项关键任务，包括：将 UI 截图转换为响应式 HTML/CSS；从图像生成可编辑的 SVG 代码（Li et al., 2025c）；解决视觉编程挑战（Li et al., 2024a）；回答多模态编码问题（例如带有图像的 StackOverflow 帖子）；以及将视觉表示（如流程图、图表和 L ATEX 方程）转录为其相应的代码或标记。这种新颖的数据混合使 Qwen3-VL 能够在视觉感知和可执行逻辑之间架起桥梁。
3.2.7 视频
Qwen3-VL 的视频理解能力得到了显著提升，能够稳健地建模帧间的时间动态、精细的空间关系感知以及超长视频序列的连贯总结。这一增强基于一个包含两项主要创新的数据处理管道：
时间感知视频理解。(i) 密集字幕合成：对于长视频序列，我们采用短到长的字幕合成策略，生成整体的、时间戳交错的和时间上连贯的故事级描述。利用内部字幕模型，我们进一步生成细粒度注释，共同捕捉事件级别的时间摘要和特定片段的视觉细节。(ii) 空间-时间视频定位：我们策划和合成了大规模的视频数据，这些数据在对象、动作和人物层面进行了标注，以加强模型的空间-时间定位能力，从而提高其细粒度视频理解的能力。
视频数据平衡和采样。(i) 来源平衡：为确保数据平衡和多样性，我们组装了一个涵盖各种视频来源的大规模数据集，包括教学内容、电影、第一人称录制等。通过元数据（如视频标题、时长和分类标签）指导的系统策划，实现了数据集的平衡。(ii) 长度自适应采样：在预训练阶段，我们根据不同的序列长度约束动态调整采样参数，如每秒帧数（fps）和最大帧数。这种自适应策略减轻了次优采样实践（如过于稀疏的帧选择或过低的空间分辨率）导致的信息损失，从而保留了视觉细节并优化了训练效果。
3.2.8 科学、技术、工程和数学（STEM）
多模态推理是 Qwen3-VL 的核心，其中 STEM 推理构成了其最重要的部分。我们的理念遵循分而治之的策略：首先独立开发细粒度的视觉感知能力和强大的语言推理能力，然后以协同方式将它们集成，以实现有效的多模态推理。
视觉感知数据。我们开发了一条专门的合成数据生成管道，通过程序化（基于代码）渲染构建几何图形。使用该管道，我们生成了：(i) 100 万个点定位样本，如交点、角点和重心；(ii) 200 万个针对图表细粒度视觉理解的视觉问答对。为了获得高保真的文本描述，我们进一步实施了两阶段字幕框架：初始生成阶段后进行严格的基于模型的验证。两个阶段均采用专门模型的集合，以确保准确性和描述的细致性。这一过程生成了一个涵盖多个 STEM 学科的 600 万个丰富注释的图表字幕综合数据集。
多模态推理数据。我们的多模态推理数据大部分由超过 60 个数据集组成。

百万 K–12 和本科水平的练习题，经过严格的清洗和重写流程精心整理。在质量过滤阶段，我们剔除低质量项目，包括图像损坏、内容无关或答案不完整或错误的题目。在重写阶段，我们在中文和英文之间翻译练习题，并标准化答案格式——如逐步解决方案列表、数学表达式和符号表示——以确保一致性和统一呈现。对于长 CoT 问题解决数据，我们合成了超过 1200 万个多模态推理样本，配以图像。为确保推理过程的连贯性和丰富性，我们利用强大的推理模型生成的原始轨迹。为保证数据的可靠性和适用性，每个样本的推理轨迹都经过严格的验证——结合基于规则的检查和基于模型的验证——并明确过滤掉包含模糊答案或语码转换的实例。此外，为提高推理质量，我们通过拒绝采样仅保留具有挑战性的问题。

语言推理数据。除了多模态推理数据外，我们还纳入了来自 Qwen3 的推理数据，因为多模态推理能力在很大程度上源自语言推理能力。

3.2.9 代理
图形用户界面 (GUI)：为了赋予 Qwen3-VL 自主与图形用户界面 (GUI) 交互的能力，我们整理和合成了涵盖桌面、移动和网络环境的大规模跨平台数据（Ye et al., 2025; Wang et al., 2025a; Lu et al., 2025）。对于 GUI 界面感知，我们利用元数据、解析工具和人工标注构建任务，如元素描述、密集字幕和密集定位，以实现对各种用户界面的稳健理解。对于代理能力，我们通过自进化轨迹生成框架组装多步骤任务轨迹，并辅以针对性的人工审核；我们还精心设计和增强链式思维理由，以加强实际执行中的规划、决策和反思性自我修正。

函数调用：为了在多模态上下文中实现通用函数调用能力，我们构建了一个多模态函数调用轨迹合成管道。首先，我们指导具备能力的模型使用图像生成用户查询及其相应的函数定义。然后，我们采样模型的函数调用及其理由，并合成函数响应。此过程重复进行，直到用户的查询被认为已解决。每一步之间，由于格式错误，轨迹可能会被过滤掉。这种管道使我们能够从大量图像中构建大规模多模态函数调用轨迹，而无需实现可执行函数。

搜索：在通用函数调用能力中，我们认为执行搜索的能力是关键，有助于在现实场景中整合长尾实体的知识。在这种情况下，我们收集了使用在线图像搜索和文本搜索工具的多模态事实查找轨迹，鼓励模型为不熟悉的实体执行搜索。通过这样做，模型学会了从网络中收集信息以生成更准确的响应。

4 后训练
4.1 训练配方
我们的后训练管道是一个三阶段过程，旨在完善模型的指令跟随能力，增强其推理能力，并使其与人类偏好对齐。每个阶段的具体数据和方法将在后续部分详细说明。

监督微调 (SFT)。第一阶段赋予模型指令跟随能力和激活潜在的推理技能。这分为两个阶段：初始阶段使用 32k 上下文长度，随后扩展到 256k 上下文窗口，专注于长文档和长视频数据。为了满足不同需求，我们将训练数据分为标准格式用于非思考模型和链式思维 (CoT) 格式用于思考模型，后者显式建模推理过程。

强到弱蒸馏。第二阶段采用知识蒸馏，其中强大的教师模型将其能力转移给我们的学生模型。重要的是，我们使用纯文本数据来微调 LLM 骨干。这种方法非常有效，显著提高了文本中心和多模态任务中的推理能力。

强化学习 (RL)。最后阶段利用 RL 进一步提升模型性能和对齐。这一阶段分为推理 RL 和通用 RL。我们应用大规模强化学习。

跨全面的文本和多模态领域进行学习，包括但不限于数学、OCR、定位和指令跟随，以提升更细粒度的能力。

4.2 冷启动数据
4.2.1 SFT 数据
我们的主要目标是赋予模型处理广泛现实场景的能力。基于 Qwen2.5-VL 的基础能力，该模型在大约八个核心领域和30个细分类别中表现出色，我们战略性地扩展了其功能范围。这一扩展通过整合社区反馈、学术文献和实际应用的见解来实现，从而引入了新的能力。这些能力包括但不限于用于具身智能的空间推理、用于细粒度视觉理解的图像定位推理、用于视频中鲁棒对象跟踪的时空定位，以及对数百页技术文档的长上下文理解。在这些目标任务的指导下，并基于真实使用案例，我们系统地策划了SFT数据集，通过精心选择和合成来自开源数据集和网络资源的样本。这一有针对性的数据工程努力对于建立 Qwen3-VL 作为更全面和强大的多模态基础模型至关重要。

该数据集包含约1,200,000个样本，战略性地组合以促进强大的多模态能力。该集合分为单模态和多模态数据，其中三分之一为纯文本条目，其余三分之二为图像-文本和视频-文本对。多模态内容的集成旨在使模型能够解释复杂的现实场景。为了确保全球相关性，数据集不仅涵盖了主要的中文和英文语料库，还包含多样化的多语言样本，从而扩大了其语言覆盖范围。此外，它通过纳入单轮和多轮对话来模拟真实的对话动态，这些对话在各种视觉设置中进行，从单张图像到多张图像序列。重要的是，数据集中还包括交错的图像-文本示例，旨在支持高级代理行为，如工具增强的图像搜索和视觉定位推理。这种异构数据组成确保了全面的覆盖，并增强了数据集的代表性，用于训练通用且复杂的多模态代理。

鉴于 Qwen3-VL 对256K令牌上下文长度的原生支持，我们采用分阶段的训练策略以优化计算效率。该策略包括两个阶段：初始阶段为一个周期的训练，序列长度为32K令牌，随后在完整256K令牌长度下进行第二个周期的训练。在后一阶段，模型在一个课程中接受训练，该课程交替使用长上下文输入和32K令牌长度的采样数据。长上下文输入包括数百页的技术文档、整本教科书和长达两小时的视频。

训练数据的质量是决定视觉-语言模型性能的关键因素。来自开源和合成来源的数据集通常存在显著的变异性和噪声，包括冗余、无关或低质量的样本。为了缓解这些缺陷，实施严格的过滤协议是必不可少的。因此，我们的数据策划过程包括一个两阶段的过滤管道：查询过滤和响应过滤。

**查询过滤**。在这一初始阶段，我们利用 Qwen2.5-VL 识别并丢弃不可验证的查询。具有模糊指令的查询会进行最小修订以提高清晰度，同时保留原始语义意图。此外，缺乏实质性内容的网络来源查询会被系统地消除。关键在于，所有剩余的查询都会进行最终的复杂性和上下文相关性评估，确保只有适当具有挑战性和相关性的样本被保留到下一阶段。

**响应过滤**。这一阶段结合了两种互补策略：
- **基于规则的过滤**：应用一组预定义的启发式方法，以消除质量不足的响应，如重复、不完整或格式不当。为了保持语义相关性和维护伦理原则，我们还会丢弃任何离题或可能生成有害内容的查询-响应对。
- **基于模型的过滤**：通过使用源自 Qwen2.5-VL 系列的奖励模型进一步精炼数据集。这些模型对多模态问答对进行多维度评估。具体来说：(a) 答案根据正确性、完整性、清晰性和有用性等标准进行评分；(b) 对于视觉定位任务，评估特别强调验证视觉信息的准确解释和利用；(c) 这种基于模型的方法能够检测出通常逃过基于规则方法的细微问题。

例如不适当的语言混用或突然的风格转变。
这一多维度过滤框架确保只有符合严格质量、可靠性和伦理完整性的数据才能进入SFT阶段。
4.2.2 长链思考冷启动数据
我们思维模型的基础是一个精心策划的长链思考（Long Chain-of-Thought, CoT）冷启动数据集，旨在激发和优化复杂的推理能力。该数据集基于纯文本和多模态数据的多样化查询集合构建，保持视觉-语言和纯文本样本的大约1:1比例，以确保技能发展的平衡。
多模态部分不仅涵盖了已建立的领域，如视觉问答（VQA）、光学字符识别（OCR）、2D/3D定位和视频分析，还特别强调与STEM和代理工作流程相关的任务。这种战略重点旨在推动模型在需要复杂、多步骤推理的问题上的表现。纯文本部分紧密模仿用于Qwen3的数据，包含数学、代码生成、逻辑推理和一般STEM领域的难题。
为保证高质量和适当难度，我们实施了严格的多阶段过滤协议。
• 难度策划：我们选择性地保留基线模型通过率低或生成较长、更详细响应的实例。这使数据集中包含了对当前模型真正具有挑战性的问题。
• 多模态必要性过滤：对于视觉-语言数学问题，我们引入了一个关键的过滤步骤：我们丢弃Qwen3-30B-nothink模型无需访问视觉输入即可正确解决的任何样本。这确保了剩余实例确实需要多模态理解，而不仅仅是通过文本线索可解。
• 响应质量控制：遵循Qwen3的方法，我们清理生成的响应。对于有多个候选答案的查询，我们首先移除包含错误最终结果的那些。随后，我们过滤掉表现出不希望模式的响应，如过度重复、不当的语言混用或显示明显猜测而缺乏充分推理步骤的答案。
这一严格的策划过程产生了一个高质量、具有挑战性的数据集，专门用于引导高级多模态推理。
4.3 强到弱蒸馏
我们采用Qwen3中描述的强到弱蒸馏管道，以进一步提高轻量级模型的性能。这一蒸馏过程包括两个主要阶段：
• 离策略蒸馏：在第一阶段，教师模型生成的输出被组合以提供响应蒸馏。这有助于轻量级学生模型获得基本的推理能力，为后续的在线策略训练奠定坚实基础。
• 在线策略蒸馏：在第二阶段，学生模型根据提供的提示生成响应。这些在线策略序列随后用于微调学生模型。我们通过最小化KL散度来对齐学生和教师预测的logits。
4.4 强化学习
4.4.1 推理强化学习
我们在多样化的文本和多模态任务上训练模型，包括数学、编程、逻辑推理、视觉定位和视觉谜题。每个任务都设计为可以通过规则或代码执行器确定性地验证解决方案。
数据准备：我们从开源和专有来源策划训练数据，并应用严格的预处理和人工标注以确保高质量的RL查询。对于多模态查询，我们使用最先进的视觉-语言模型（Qwen3-VL-235B-A22B）的初步检查点，每条查询采样16个响应；任何所有响应均为错误的查询将被丢弃。
11

我们针对每项任务进行初步的强化学习（RL）实验，以识别并移除改进潜力有限的数据源。这一过程产生了大约30K个涵盖多种文本和多模态任务的RL查询。对于每个模型的训练，我们为所有查询采样16个响应，并过滤掉通过率超过90%的简单查询。我们将特定任务的数据集打乱并合并，构建混合任务批次，确保每个任务的样本比例一致且预定义。该比例是通过广泛的初步实验确定的。

奖励系统 我们实现了一个统一的奖励框架，以提供跨所有任务的精确反馈。该系统提供了共享的基础设施——数据预处理、实用函数和奖励管理器，用于整合多种奖励类型——而核心奖励逻辑则按任务实现。我们使用特定任务的格式提示来引导模型输出到所需格式，因此不依赖于显式的格式奖励。为了减轻代码切换，当响应语言与提示语言不同时，我们施加惩罚。

RL算法 我们采用SAPO（Gao et al., 2025），这是一种平滑且自适应的策略梯度方法，用于RL训练。SAPO在不同的文本和多模态任务以及不同模型大小和架构中均表现出一致的改进。

4.4.2 通用强化学习
通用强化学习（RL）阶段旨在增强模型的泛化能力和操作鲁棒性。为此，我们采用多任务RL范式，其中奖励函数基于SFT阶段的一系列全面任务制定，包括视觉问答（VQA）、图像描述、OCR、文档解析、定位和时钟识别。奖励机制旨在优化模型性能的两个主要维度：
• 指令遵循：此维度评估模型对用户明确指令的遵循情况。它评估处理内容、格式、长度和结构化输出（例如JSON）的复杂约束的能力，确保生成的响应精确符合用户要求。
• 偏好对齐：对于开放性或主观性查询，此维度通过优化有用性、事实准确性和风格适当性，使模型输出与人类偏好对齐。这促进了更自然和引人入胜的用户互动。

此外，这一阶段作为纠正机制，用于消除SFT期间内化的强烈但有缺陷的知识先验。我们通过引入专门的、可验证的任务来解决这一问题，这些任务旨在触发这些特定错误，例如反直觉的对象计数和复杂的时钟时间识别。这种有针对性的干预旨在用事实知识替代错误的先验。

另一个关键目标是减少不当的语言混用、过度重复和格式错误等劣质行为。然而，这些问题的低发生率使得通用RL成为一种样本效率低下的纠正策略。为克服这一点，我们在这一阶段策划了一个专用数据集。该数据集隔离了已知会引发这些不良行为的提示。这种集中训练使我们能够应用有针对性的高频惩罚，有效抑制这些残留错误。

RL过程的反馈通过结合两种互补方法的混合奖励系统提供：
• 基于规则的奖励：这种方法为具有可验证真实值的任务（如格式遵循和指令遵循）提供明确、高精度的反馈。通过使用明确定义的启发式方法，该方法提供了一种强大的机制来评估正确性，并有效防止模型利用学习到的奖励函数中的模糊性。
• 基于模型的奖励：这种方法使用Qwen2.5-VL-72B-Instruct或Qwen3作为复杂的评判模型。评判模型将每个生成的响应与真实参考进行比较，从多个方面评分其质量。这种方法在评估需要灵活处理的细微或开放性任务时特别有效，特别是在最小化因非传统格式或措辞而被误罚的有效响应方面。

4.5 用图像思考
受“用图像思考”领域的杰出前期工作（Wu et al., 2025a; Jin et al., 2025; Zheng et al., 2025; Lai et al., 2025）的启发，我们通过两阶段训练范式赋予Qwen3-VL类似的代理能力。

12

在第一阶段，我们合成了一个冷启动代理数据集，包含大约10k个基础示例——主要是简单的两轮视觉问答任务，如属性检测。然后，我们在Qwen2.5-VL-32B上进行监督微调（SFT），以模拟视觉代理的行为：思考→行动→分析反馈→回答。为了进一步增强其推理能力，我们应用了多轮、工具集成的强化学习（RL）。

在第二阶段，我们将第一阶段训练的Qwen2.5-VL-32B视觉代理进行蒸馏，生成一个更大、更多样化的数据集，包含约120k个多轮代理交互，涵盖更广泛的视觉任务。然后，我们应用类似的冷启动SFT和工具集成的RL管道（现在使用蒸馏和合成的数据）对Qwen3-VL进行后训练。

多轮、工具集成的RL过程在这两个阶段中几乎相同，只是底层数据不同。在RL过程中，我们采用三种互补的奖励信号来鼓励稳健的、工具介导的推理：
• 答案准确性奖励利用Qwen3-32B来衡量最终答案是否正确。
• 多轮推理奖励利用Qwen2.5-VL-72B来评估助手是否正确解释工具或环境反馈，并通过连贯的逐步推理得出答案。
• 工具调用奖励通过比较实际的工具调用次数与专家估计的目标来鼓励适当的工具使用。该目标由Qwen2.5-VL-72B根据任务复杂度离线确定。

早期实验显示，模型倾向于只进行一次工具调用来获取前两种奖励，而不管任务需求如何。为了解决这一问题，我们明确纳入工具调用奖励，以促进与任务复杂度一致的适应性工具探索。

4.6 基础设施
我们在阿里云的PAI-Lingjun AI计算服务上训练Qwen3-VL系列模型，该服务提供了计算密集型场景（如AI和高性能计算）所需的高性能计算能力。

在预训练阶段，系统采用基于Megatron-LM框架的混合并行策略，集成了张量并行（TP）、流水线并行（PP）、上下文并行（CP）、专家并行（EP）和ZeRO-1数据并行（DP）。这种配置在模型规模、计算负载和通信开销之间实现了细粒度的平衡，使硬件利用率高，并且即使在高达10,000个GPU的规模下也能保持高吞吐量和低通信延迟。

对于本地部署和性能评估，我们采用了基于vLLM或SGLang的部署策略。vLLM利用PagedAttention实现内存高效的管理和高吞吐量推理，而SGLang在结构化生成和处理复杂提示方面表现出色。这些后端共同提供了高效推理和评估的能力，具有稳定、高效和灵活的模型推理能力。

5 评估
5.1 通用视觉问答
为了全面评估Qwen3-VL系列的通用视觉问答（VQA）能力，我们在多个基准测试集上进行了广泛的评估，包括MMBench-V1.1（Liu et al., 2023b）、RealWorldQA（xAI, 2024）、MMStar（Chen et al., 2024a）和SimpleVQA（Cheng et al., 2025）。如表2、表3和表4所示，Qwen3-VL家族在从2B到235B参数的各种模型规模下表现出稳健且高度竞争力的性能。

在思维模式的对比中，Qwen3-VL-235B-A22B-Thinking在MMStar上取得了最高分78.7。Gemini-2.5-Pro（Comanici et al., 2025）的思维模式总体表现最佳，但Qwen3-VL-235B-A22B-Thinking紧随其后。在非推理模式的对比中，Qwen3-VL-235B-A22B-Instruct在MMBench和RealWorldQA上分别获得了89.3/88.9和79.2的最高分。

在中等规模模型的实验中，Qwen3-VL-32B-Thinking在MMBench和RealWorldQA上分别获得了89.5/89.5和79.4的最高分。值得注意的是，Qwen3-VL-32B-Instruct 13

甚至在 RealWorldQA 上的表现也超过了 Thinking 变体，得分为 79.0。
Qwen3-VL 系列的可扩展性在其较小模型的强劲表现中显而易见。
具体来说，最大的模型 Qwen3-VL-8B 在所有五个基准测试中均取得了最高性能。
例如，在 MMBench-EN 上，“思考”模式下的得分从 2B 模型的 79.9 提高到 8B 模型的 85.3。
在其他基准测试中也观察到了类似的上升趋势，例如在 MMStar 上，得分从 68.1（2B，思考）提高到 75.3（8B，思考）。

5.2 多模态推理
我们对 Qwen3-VL 系列进行了广泛的多模态推理基准测试评估，主要集中在 STEM 相关任务和视觉谜题上，包括 MMMU (Yue et al., 2024a)、MMMU-Pro (Yue et al., 2024b)、MathVision (Wang et al., 2024b)、MathVision-Wild photo（以下简称 MathVision WP）、MathVista (Lu et al., 2023)、We-Math (Qiao et al., 2024)、MathVerse (Zhang et al., 2024)、DynaMath (Zou et al., 2024)、Math-VR (Duan et al., 2025)、LogicVista (Xiao et al., 2024)、VisualPuzzles (Song et al., 2025b)、VLM are Blind (Rahmanzadehgervi et al., 2025)、ZeroBench (Main/Subtasks) (Roberts et al., 2025) 和 VisuLogic (Xu et al., 2025)。如表 2 所示，旗舰 Qwen3-VL 模型在“非思考”和“思考”模型中均表现出色。特别是，Qwen3-VL-235B-A22B-Instruct 在多个基准测试中，包括 MathVista mini、MathVision、MathVerse mini、DynaMath、ZeroBench、VLMsAreBlind、VisuLogic 和 VisualPuzzles Direct 中，取得了非思考或低思考预算模型中的最佳报告结果。而 Qwen3-VL-235B-A22B-Thinking 在 MathVista mini、MathVision、MathVerse mini、ZeroBench、LogicVista 和 VisuLogic 中取得了最先进的结果。

在中等规模模型中，如表 3 所示，Qwen3-VL-32B 表现出了显著的优势，持续超越 Gemini-2.5-Flash 和 GPT-5-mini。与前一代 Qwen2.5-VL-72B 模型相比，中等规模的 Qwen3-VL 模型已经在推理任务上超越了它。这突显了 VLM 的显著进展。此外，我们新推出的 Qwen3-VL-30B-A3B MoE 模型也取得了有竞争力的结果。

在小规模模型中，我们将 Qwen3-VL-2B/4B/8B 与 GPT-5-Nano 进行了比较，结果如表 4 所示。8B 变体总体上保持了明显优势，而 4B 模型在 DynaMath 和 VisuLogic 上取得了最高分数。值得注意的是，即使是最小的 2B 模型也表现出强大的推理能力。

5.3 对齐和主观任务
当前大型视觉语言模型（VLM）能够遵循复杂的用户指令并减少潜在的图像级幻觉是不可或缺的。我们在三个代表性基准测试中评估了我们的模型：MM-MT-Bench (Agrawal et al., 2024)、HallusionBench (Guan et al., 2023) 和 MIA-Bench (Qian et al., 2024)。MM-MT-Bench 是一个多轮次的 LLM 作为裁判的评估基准，用于测试多模态指令调优模型。HallusionBench 旨在诊断图像上下文推理，对当前的 VLM 构成了巨大挑战。MIA-Bench 是一个更全面的基准，用于评估模型对用户复杂指令的反应（例如，有限字数的创意写作和组合指令）。

如表 2 所示，我们的旗舰 Qwen3-VL-235B-A22B 模型始终优于其他闭源模型。在 HallusionBench 上，我们的思考版本分别超过了 Gemini-2.5-pro (Comanici et al., 2025)、GPT-5 (OpenAI., 2025) 和 Claude opus 4.1 (Anthropic., 2025) 3.0、1.0 和 6.3 分。在 MIA-Bench 上，Qwen3-VL-235B-A22B-Thinking 在所有其他模型中取得了整体最佳分数，展示了我们卓越的多模态指令跟随能力。我们还调查了 MIA-Bench 的详细子任务结果：我们的模型在 MIA-Bench 的数学和文本子任务中分别超过了 GPT-5 高思考版本 10.0 和 5.0 分。同样的趋势在我们较小规模的模型如 Qwen3-VL-30B-A3B 和 Qwen3-VL-32B 中也可以观察到，它们在与其他相似规模的模型对比中也超过了其他模型。我们的 2B/4B/8B 系列也表现出色，并且在 MIA-Bench 上的下降幅度可以忽略不计。

5.4 文本识别和文档理解
我们将在文档相关基准测试中将 Qwen3-VL 系列与其他规模相当的模型进行比较，包括光学字符识别（OCR）、文档解析、文档问答（QA）和文档推理。
我们评估了我们的旗舰模型 Qwen3-VL-235B-A22B，与最先进的 VLM 在表 2 列出的基准测试中进行了对比。在以 OCR 为重点的解析基准测试——包括 CC-OCR (Yang et al., 2024b) 和 OmniDocBench (Ouyang et al., 2024)——以及综合性的 OCR 基准测试如 OCR-Bench (Liu et al., 2024) 和 OCRBench_v2 (Fu et al., 2024b) 中，Qwen3-VL-235B-A22B-Instruct 模型 14

表2：Qwen3-VL-235B-A22B 和顶级模型在视觉基准测试中的表现。推理模型和非推理模型的最高分数分别以粗体和下划线显示。
结果标有∗的数据来自技术报告。+表示使用工具的结果。

基准
思考指令 思考 预算-128 高 最小 思考 非思考
STEM
PuzzleMMMU 80.6 78.7 81.7∗ 80.9 84.2∗ 74.4∗ 78.4 77.2
MMMU-Pro 69.3 68.1 68.8∗ 71.2 78.4∗ 62.7∗ 64.8 60.7
MathVista mini 85.8 84.9 82.7∗ 77.7 81.3 50.9 75.5 74.5
MathVision 74.6 66.5 73.3∗ 66.0 70.9 45.8 64.3 57.7
MathVision WP 63.8 57.0 63.2 56.9 62.8 40.1 54.0 46.4
We-Math 74.8 67.5 80.6 74.5 73.8 51.8 65.2 60.2
MathVerse mini 85.0 72.5 82.9 65.9 84.1 43.0 70.6 68.1
DynaMath 82.8 79.4 80.0 78.5 85.4 74.0 75.1 72.0
Math-VR 66.8 65.0 64.7* 54.3 58.1 21.7 54.3 38.0
ZeroBench 4 2 3 1 2 2 3 1
VlmsAreBlind 79.5 80.4 86.1 78.5 80.5 53.4 77.8 72.2
LogicVista 72.2 65.8 72.0 68.7 71.8 46.3 67.3 63.5
VisuLogic 34.4 29.9 31.6 26.9 28.5 27.2 27.9 27.2
VisualPuzzles 57.2 54.7 60.9 56.9 57.3 47.9 48.8 47.6
通用 VQA
MMBench-EN 88.8 89.3 90.1∗ 88.4 83.8 81.3 79.4 83.0
MMBench-CN 88.6 88.9 89.7∗ 86.4 83.5 79.9 84.9 74.3
RealWorldQA 81.3 79.2 78.0∗ 76.0 82.8 77.3 69.9 68.5
MMStar 78.7 78.4 77.5∗ 78.5 76.4 65.2 72.1 71.0
SimpleVQA 61.3 63.0 65.4 66.9 61.8 56.7 56.7 55.7
AlignmentHallusionBench 66.7 63.2 63.7∗ 60.9 65.7 53.7 60.4 55.1
MM-MT-Bench 8.5 8.5 8.4∗ 7.6 7.6 7.5 7.8 7.9
MIA-Bench 92.7 91.3 92.3 91.3 92.4 92.6 91.2 90.0
文档理解
DocVQA 测试 96.5 97.1 92.6 94.0 91.5 89.6 92.5 89.2
InfoVQA 测试 89.5 89.2 84.2 82.9 79.0 69.9 69.4 60.9
AI2D w. M. 89.2 89.7 90.9 90.0 89.7 84.1 86.4 84.4
ChartQA 测试 90.3 90.3 83.3 62.6 59.7 59.1 86.2 83.9
OCRBench 875 920 866 872 810 787 764 750
OCRBench_v2 en 66.8 67.1 54.3 55.2 53.0 48.2 48.4 47.2
OCRBench_v2 zh 63.5 61.8 48.5 53.1 43.2 37.7 43.7 38.0
CC-OCR 81.5 82.2 77.2 76.8 68.3 66.1 69.1 66.0
OmniDocBench en 0.155 0.143 0.347 0.206 0.356 0.174 0.194 -
OmniDocBench zh 0.207 0.207 0.238 0.249 0.472 0.389 0.293 -
CharXiv(DQ) 90.5 89.4 94.4 87.8 89.2 79.5 88.5 87.8
CharXiv(RQ) 66.1 62.1 67.9 62.9 81.1∗ 57.8 63.6 60.2
MMLongBench Doc 56.2 57.0 55.6 51.2 51.5 42.4 54.5 48.1
2D/3D
GroundingRefCOCO-avg 92.1 91.9 74.6∗ - 66.8 - - -
CountBench 93.7 93.0 91.0∗ 91.0 91.7 87.8 93.1 91.9
ODinW-13 43.2 48.6 33.7∗ 34.5 - - - -
ARKitScenes 53.7 56.9 - - - - - -
Hypersim 11.0 13.0 - - - - - -
SUNRGBD 34.9 39.4 29.7 - - - - -
具身/空间理解
ERQA 52.5 51.3 55.3 50.3 65.7∗ 42.0∗ 34.8 28.0
VSI-Bench 60.0 62.7 - - - - - -
EmbSpatialBench 84.3 83.1 79.1 73.3 82.9 75.1 69.2 66.0
RefSpatialBench 69.9 65.5 36.5 35.6 23.8 23.1 - -
RoboSpatialHome 73.9 69.4 47.5 49.2 53.5 43.6 - -
多图像
BLINK 67.1 70.7 70.6∗ 70.0 71.0 62.8 64.1 62.9
MUIRBENCH 80.1 73.0 77.2 74.0 77.5 66.5 - -
视频理解
MVBench 75.2 76.5 69.9 65.8 75.3 64.6 61.4 59.0
Video-MME w/o sub. 79.0 79.2 85.1 80.6 84.7 77.3 75.6 73.3
MLVU M-Avg 83.8 84.3 85.6 81.2 86.2 78.3 73.5 71.2
LVBench 63.6 67.7 73.0 69.0 - - - -
Charades-STA mIoU 63.5 64.8 - - - - - -
VideoMMMU 80.0 74.7 83.6∗ 79.4 84.6∗ 61.6∗ 76.2 70.1
MMVU 71.1 68.1 74.9 72.2 73.0 68.1 66.4 61.4
感知
使用工具 V∗ 85.9 93.7+ 83.8 72.7 72.8 56.7 - -
HRBench4K 84.3 85.4+ 87.3 84.8 - - - -
HRBench8K 76.6 82.4+ 85.4 80.1 - - - -
多模态
编码设计 Design2Code 93.4 92.0 89.2 90.3 92.5 88.9 88.5 85.3
ChartMimic 78.4 80.5 83.9 79.9 62.1 41.4 85.2 82.9
UniSVG 65.8 69.8 70.0 67.9 71.7 74.5 73.0 72.5
多模态代理
ScreenSpot Pro 61.8 62.0 - - - - - -
OSWorldG 68.3 66.7 45.2 - - - - -
AndroidWorld 62.0 63.7 - - - - - -
OSWorld 38.1 31.6 - - - - - 44.4
WindowsAA 32.1 28.9 - - - - - -Qwen3-VL
235B-A22BGemini
2.5 ProOpenAI
GPT-5Claude
Opus 4.1
15

表3：中等规模 Qwen3-VL 模型与先前模型在视觉基准测试中的表现。
最高分数以粗体显示。带有 ∗ 标记的结果来自技术报告。 + 表示使用工具的结果。
基准测试
思考指令 思考指令 非思考 高 最低
STEM
PuzzleMMMU 76.0 74.2 78.1 76.0 77.7 76.3 79.0 67.9
MMMU-Pro 63.0 60.4 68.1 65.3 67.2 65.9 67.3 53.7
MathVista mini 81.9 80.1 85.9 83.8 79.4 75.3 79.1 59.6
MathVision 65.7 60.2 70.2 63.4 64.3 60.7 71.9 46.6
MathVision WP 58.9 52.3 58.6 54.6 53.6 49.0 56.6 42.8
We-Math 70.0 56.9 71.6 63.3 53.9 60.3 70.2 51.4
MathVerse mini 79.6 70.2 82.6 76.8 77.7 75.9 78.8 36.5
DynaMath 80.1 73.4 82.0 76.7 75.9 69.7 81.4 71.3
Math-VR 61.7 61.3 62.3 59.8 58.8 54.7 58.2 26.4
ZeroBench 0 0 2 1 1 3 3 2
VlmsAreBlind 72.5 67.5 85.1 87.0 77.5 75.9 75.8 62.0
LogicVista 65.8 53.5 70.9 62.2 67.3 60.0 71.4 50.8
VisuLogic 26.6 23.0 32.4 29.7 31.0 23.3 27.2 27.6
VisualPuzzles 52.0 46.2 54.7 53.2 41.4 45.0 59.3 48.2
通用 VQA
MMBench-EN 87.0 86.1 89.5 87.6 87.1 86.6 86.6 78.5
MMBench-CN 85.9 85.3 89.4 87.7 87.3 86.0 84.0 76.3
RealWorldQA 77.4 73.7 78.4 79.0 76.0 75.7 79.0 73.3
MMStar 75.5 72.1 79.4 77.7 76.5 75.8 74.1 61.3
SimpleVQA 54.3 52.7 55.4 56.9 63.2 59.2 56.8 50.3
AlignmentHallusionBench 66.0 61.5 67.4 63.8 63.5 59.1 63.2 55.9
MM-MT-Bench 7.9 8.0 8.3 8.4 8.1 8.0 7.7 7.4
MIA-Bench 91.6 91.2 92.3 91.8 91.1 90.6 92.0 92.3
文档理解
DocVQA 测试 95.5 95.0 96.1 96.9 92.8 93.0 90.5 90.6
InfoVQA 测试 85.6 81.8 89.2 87.0 82.5 81.7 77.6 72.8
AI2D w. M. 86.9 85.0 88.9 89.5 88.7 87.7 88.2 82.9
ChartQA 测试 89.4 86.8 89.0 88.5 60.6 69.0 57.5 57.8
OCRBench 839 903 855 895 853 864 821 807
OCRBench_v2 en 62.6 63.2 68.4 67.4 52.2 50.6 52.6 45.7
OCRBench_v2 zh 60.4 57.8 62.1 59.2 43.8 43.9 45.1 41.0
CC-OCR 77.8 80.7 79.6 80.3 75.4 74.8 70.8 61.6
OmniDocBench en 0.165 0.183 0.148 0.151 0.265 0.228 0.181 0.260
OmniDocBench zh 0.233 0.253 0.236 0.239 0.245 0.305 0.316 0.425
CharXiv(DQ) 86.9 85.5 90.2 90.5 90.1 85.5 89.4 78.6
CharXiv(RQ) 56.6 48.9 65.2 62.8 61.7 60.1 68.6 48.9
MMLongBench 文档 47.4 47.1 54.6 55.4 49.0 44.6 50.3 39.6
2D/3D
GroundingRefCOCO-avg 89.3 89.7 91.1 91.9 - - - -
CountBench 90.0 89.8 94.1 94.9 86.0 83.7 91.0 84.1
ODinW-13 42.3 47.5 41.8 46.6 - - - -
ARKitScenes 55.6 56.1 46.1 55.6 - - - -
Hypersim 11.4 12.5 12.5 14.0 - - - -
SUNRGBD 34.6 38.1 33.9 37.0 - - - -
具身/空间理解
ERQA 45.3 43.0 52.3 48.8 - - 54.0 45.8
VSI-Bench 56.1 63.2 61.2 61.5 - - 31.5 30.5
EmbSpatialBench 80.6 76.4 82.7 81.5 - - 80.7 72.1
RefSpatialBench 54.2 53.1 67.2 61.4 - - 9.0 4.0
RoboSpatialHome 65.5 62.9 74.2 64.6 - - 54.3 44.6
多图像
BLINK 65.4 67.7 68.5 67.3 68.1 66.8 - 56.7
MUIRBENCH 77.6 62.9 80.3 72.8 72.7 67.5 - 57.5
视频
理解
MVBench 72.0 72.3 73.2 72.8 - - - -
Video-MME 无字幕 73.3 74.5 77.3 76.6 79.6 75.6 78.9 71.0
MLVU M-Avg 78.9 81.3 82.3 82.1 82.1 77.8 83.3 71.7
LVBench 59.2 62.5 62.6 63.8 64.5 62.2 - -
Charades-STA mIoU 62.7 63.5 62.8 61.2 - - - -
VideoMMMU 75.0 68.7 79.0 71.9 73.9 65.2 82.5* 56.7
MMVU 66.1 59.8 67.9 66.8 69.8 68.2 69.8 64.8
感知
使用工具
V∗81.2 89.5+ 84.8 91.1+ - 78.6 63.9
HRBench4K 77.8 82.5+ 82.1 84.6+ - 78.6 66.3
HRBench8K 71.3 79.3+ 74.8 81.6+ - 74.4 60.9
多模态
代理
ScreenSpot Pro 57.3 60.5 57.1 57.9 - - - -
OSWorldG 59.6 61.0 64.0 65.1 - - - -
AndroidWorld 55.0 54.3 63.7 57.3 - - - -
OSWorld 30.6 30.3 41.0 32.6 - - - -
WindowsAA 24.2 24.9 42.9 30.9 - - - -
Qwen3-VL
30B-A3B Qwen3-VL
32B Gemini
2.5 FlashGPT-5
mini
16

图2：我们的模型在自建测试集上的多语言OCR性能。该模型在39种支持的语言中有32种达到了超过70%的准确率，展示了强大的多语言能力。
该模型建立了一个新的最先进水平，略微超过了其“思考”版本Qwen3-VL-235B-A22B-Thinking。在需要OCR能力和关键词搜索的OCR相关视觉问答（VQA）基准测试中——例如DocVQA（Mathew等，2021b）、InfoVQA（Mathew等，2021a）、AI2D（Kembhavi等，2016）、ChartQA（Masry等，2022）以及CharXiv（Wang等，2024g）的描述子集——指令版和思考版都表现出相当的性能，展示了在这些任务上的一致强表现。值得注意的是，在要求深入图表理解和多步骤推理的CharXiv推理子集中，思考版超过了指令版，并且仅次于GPT5-thinking和Gemini-2.5-Pro-Thinking，排名第二。
此外，在Qwen3-VL系列中的较小规模变体中，Qwen3-VL-30BA3B模型和Qwen3-VL-32B模型在大多数评估指标上均优于Gemini-2.5-Flash和GPT-5-mini，如表3所示。即使是紧凑的密集模型——Qwen3-VL-8B、Qwen3-VL-4B和Qwen3-VL-2B——在OCR解析、视觉问答（VQA）和综合基准套件上也表现出非常有竞争力的性能，详见表4。这突显了Qwen3-VL架构在不同模型规模下的卓越效率和强大可扩展性。
在这一版本的Qwen3-VL中，我们特别强调增强其理解长文档的能力。如表2所示，在MMLongBench-Doc基准测试（Ma等，2024）中与旗舰模型的比较中，我们的Qwen3-VL-235B-A22B在指令/思考设置下分别实现了57.0%/56.2%的整体准确率，展示了在长文档理解任务上的最先进性能。
除了在已建立的基准测试中的强劲表现外，我们在多语言支持方面也取得了重大进展。这标志着从Qwen2.5-VL支持的10种非英语/中文语言扩展到Qwen3-VL支持的39种语言的重大扩展。我们在新构建的内部数据集上评估了这种扩展能力。如图2所示，该模型在39种测试语言中的32种上准确率超过了70%——这是我们认为适用于实际应用的阈值。这表明Qwen3-VL的强大OCR能力不仅限于少数几种语言，而是扩展到了广泛多样的语言谱系。
5.5 2D和3D定位
在本节中，我们对Qwen3-VL系列在2D和3D定位相关基准测试中的表现进行全面评估，并将其与具有类似能力的最先进模型进行比较。
我们在指代表达理解基准测试RefCOCO/+/g（Kazemzadeh等，2014；Mao等，2016）、开放词汇对象检测基准测试ODinW-13（Li等，2022）以及计数基准测试CountBench（Paiss等，2023）上评估了Qwen3-VL的2D定位能力。对于17

表4：小型 Qwen3-VL 模型和 GPT-5-nano 在视觉基准测试中的表现。
基准
思考指令 思考指令 思考指令 高 最低
STEM
PuzzleMMMU 61.4 53.4 70.8 67.4 74.1 69.6 75.8 57.6
MMMU-Pro 42.5 36.5 57.0 53.2 60.4 55.9 57.2 36.5
MathVista mini 73.6 61.3 79.5 73.7 81.4 77.2 71.5 40.9
MathVision 45.9 31.6 60.0 51.6 62.7 53.9 62.2 33.2
MathVision WP 35.5 30.9 48.7 44.4 53.3 45.4 49.3 28.3
MathVerse mini 66.9 52.1 75.2 46.8 77.7 62.1 74.2 27.0
DynaMath 66.7 54.2 74.4 65.3 73.2 67.7 78.0 62.0
Math-VR 37.7 20.7 58.1 52.3 59.0 53.4 49.7 25.0
ZeroBench 0 0 0 0 2 1 1 1
VlmsAreBlind 50.0 56.0 68.6 71.9 69.1 74.0 66.7 40.2
LogicVista 50.0 35.8 61.1 53.2 65.1 55.3 59.7 40.5
VisuLogic 25.4 11.5 30.2 19.0 27.5 22.5 24.5 24.0
VisualPuzzles 37.4 34.3 48.9 43.7 51.7 47.9 43.5 31.3
通用 VQA MMBench-EN 79.9 78.4 84.6 83.9 85.3 84.5 78.4 50.8
MMBench-CN 78.8 75.9 83.8 83.5 85.5 84.7 77.6 48.5
RealWorldQA 69.5 63.9 73.2 70.9 73.5 71.5 71.8 60.7
MMStar 68.1 58.3 73.2 69.8 75.3 70.9 68.6 41.3
SimpleVQA 43.6 40.7 48.8 48.0 49.6 50.2 46.0 39.0
AlignmentHallusionBench 54.9 51.4 64.1 57.6 65.4 61.1 58.4 39.3
MM-MT-Bench 6.9 5.9 7.7 7.5 8.0 7.7 6.6 6.2
MIA-Bench 85.6 83.6 91.0 89.7 91.5 91.1 89.9 89.6
文档
理解 DocVQA 测试 92.9 93.3 94.2 95.3 95.3 96.1 88.2 78.3
InfoVQA 测试 77.1 72.4 83.0 80.3 86.0 83.1 68.6 49.2
AI2D 带 M. 80.4 76.9 84.9 84.1 84.9 85.7 81.9 65.7
ChartQA 测试 86.6 79.1 88.8 84.6 88.6 89.6 52.1 48.6
OCRBench 792 858 808 881 819 896 753 701
OCRBench_v2 英文 56.4 56.3 61.8 63.7 63.9 65.4 48.1 37.9
OCRBench_v2 中文 51.9 53.0 55.8 57.6 59.2 61.2 33.6 27.3
CC-OCR 68.3 72.8 73.8 76.2 76.3 79.9 58.9 52.9
OmniDocBench 英文 0.370 0.292 0.234 0.244 0.209 0.170 0.401 0.454
OmniDocBench 中文 0.447 0.348 0.297 0.285 0.253 0.264 0.518 0.568
CharXiv(DQ) 70.1 62.3 83.9 76.2 85.9 83.0 82.0 64.4
CharXiv(RQ) 37.1 26.8 50.3 39.7 53.0 46.4 50.1 31.7
MMLongBench 文档 33.8 31.6 44.4 43.5 48.0 47.9 31.8 22.1
2D/3D
GroundingRefCOCO-avg 84.8 85.6 88.2 89.0 88.2 89.1 - -
CountBench 84.1 88.4 89.4 84.9 91.5 80.5 80.0 62.9
ODinW-13 36.0 43.4 39.4 48.2 39.8 44.7 - -
ARKitScenes 47.7 56.2 46.3 56.6 46.6 56.8 - -
Hypersim 11.2 12.0 11.9 12.2 12.0 12.7 - -
SUNRGBD 28.6 33.8 28.0 34.7 30.4 36.2 - -
具身/空间
理解 ERQA 41.8 28.3 47.3 41.3 46.8 45.8 45.8 37.8
VSI-Bench 48.0 53.9 55.2 59.3 56.6 59.4 15.4 27.0
EmbSpatialBench 75.9 69.2 80.7 79.6 81.1 78.5 74.2 50.7
RefSpatialBench 28.9 30.3 45.3 46.6 44.6 54.2 12.6 2.5
RoboSpatialHome 45.3 49.1 63.2 61.7 62.0 66.9 46.1 44.8
多图像 BLINK 57.2 53.8 63.4 65.8 64.7 69.1 58.3 42.2
MUIRBENCH 68.1 47.4 75.0 63.8 76.8 64.4 65.7 45.7
视频
理解 MVBench 64.5 61.7 69.3 68.9 69.0 68.7 - -
Video-MME 无字幕 62.1 61.9 68.9 69.3 71.8 71.4 66.2 49.4
MLVU M-Avg 69.2 68.3 75.7 75.3 75.1 78.1 69.2 52.6
LVBench 47.6 47.4 53.5 56.2 55.8 58.0 - -
Charades-STA mIoU 56.9 54.5 59.0 55.5 59.9 56.0 - -
VideoMMMU 54.1 41.9 69.4 56.2 72.8 65.3 63.0 40.2
MMVU 48.9 41.7 58.6 50.5 62.0 58.7 63.1 51.0
感知
使用工具 V∗69.1 75.9+74.9 88.0+77.5 90.1+- -
HRBench4K 69.4 72.6+73.5 81.3+72.4 82.3+- -
HRBench8K 62.6 68.9+67.1 74.4+68.1 78.0+- -
多模态
代理 ScreenSpot Pro 32.2 48.5 49.2 59.5 46.6 54.6 - -
OSWorldG 41.8 46.1 53.9 58.2 56.7 58.2 - -
AndroidWorld 46.1 36.4 52.0 45.3 50.0 47.6 - -
OSWorld 19.0 17.0 31.4 26.2 33.9 33.9 - -
WindowsAA - - 35.5 23.4 24.1 28.8 - -Qwen3-VL
2BQwen3-VL
4BQwen3-VL
8BOpenAI
GPT-5 nano
18

在 ODinW-13 中，我们采用平均精度均值（mean Average Precision, mAP）作为评估指标，并将置信度分数设置为 1.0。为了确保与传统开放集目标检测专业模型的可比性，我们在评估期间同时在提示中提供所有数据集类别。如表 2 所示，我们的旗舰模型 Qwen3-VL-235B-A22B 展现了卓越的性能，在 2D 定位和计数基准测试中取得了最先进的（SOTA）结果。值得注意的是，它在 ODinW-13 上达到了 48.6 mAP，展示了在多目标开放词汇对象定位方面的强大性能。我们较小规模变体的详细结果也表现出在 2D 视觉定位方面的竞争力，分别见表 3 和表 4。

此外，在此版本的 Qwen3-VL 中，我们增强了其空间感知能力，用于 3D 对象定位。我们在 Omni3D（Brazil 等，2023）上评估了 Qwen3-VL 系列与其他规模相当的模型。Omni3D 是一个综合基准，包含 ARKitScenes（Baruch 等，2021）、Hypersim（Roberts 等，2021）和 SUN RGB-D（Song 等，2015）等数据集。我们采用平均精度均值（mAP）作为评估指标。每个输入是一个图像-文本对，包括图像和指定对象类别的文本提示。为了确保与现有视觉语言模型（VLMs）的公平比较，我们将交并比（IoU）阈值设置为 0.15，并报告 Omni3D 测试集上的 mAP@0.15，检测置信度固定为 1.0。如表 2 所示，我们的旗舰模型 Qwen3-VL-235B-A22B 在多个数据集上始终优于其他闭源模型。具体而言，在 SUN RGB-D 数据集（Song 等，2015）上，Qwen3-VL-235B-A22B-Thinking 变体的性能超过了 Gemini-2.5-Pro 5.2 个百分点。我们的较小规模变体（例如 Qwen3-VL-30BA3B、-32B、-8B、-4B、-2B）在 3D 对象定位方面也表现出显著的竞争性能，详细结果分别见表 3 和表 4。

5.6 细粒度感知
我们在三个流行的基准上测量了模型的细粒度感知能力。Qwen3-VL 系列在细粒度视觉理解方面相比其前身 Qwen2.5-VL-72B 实现了显著的飞跃。特别是，Qwen3-VL-235B-A22B 在工具增强的情况下，在所有三个基准上都达到了最先进的性能——在 V*（Wu & Xie，2024）上达到 93.7，在 HRBench-4k（Wang 等，2024e）上达到 85.3，在 HRBench-8k（Wang 等，2024e）上达到 82.3。这种持续的优越表现突显了 Qwen3-VL 架构改进和训练策略的有效性，特别是在处理高分辨率输入和细微视觉差异方面，这些对于细粒度感知任务至关重要。其次，或许更令人惊讶的是，集成外部工具带来的性能提升始终超过单纯增加模型规模带来的提升。例如，在 Qwen3-VL 系列中，添加工具的绝对提升在 V* 上始终约为 5 个百分点。这些发现进一步强化了我们对多模态中扩展工具集成代理学习的信心，认为这是一条极具前景的发展路径。

5.7 多图像理解
除了单图像定位对话评估外，推进视觉语言模型（VLMs）处理多图像理解具有重要意义。这一任务需要跨不同视觉模式进行更高层次的情境分析，从而实现更高级的识别和推理能力。为此，我们通过全面的跨图像模式学习技术来增强 Qwen3-VL，包括多图像指代定位、视觉对应关系和多步推理。我们在两个主要的多图像基准上评估了 Qwen3-VL：BLINK（Fu 等，2024c）和 MuirBench（Wang 等，2024a）。如表 2 所示，Qwen3-VL 在多图像理解方面总体上优于其他领先的大型视觉语言模型（LVLMs）。具体而言，Qwen3-VL-235B-A22B-Instruct 的性能与最先进的模型如 Gemini-2.5-pro 相当，而 Qwen3-VL-235B-A22B-Thinking 在 MuirBench 上取得了 80.1 的显著领先分数，超越了所有其他模型。

5.8 身体和空间理解
在身体和空间理解方面，Qwen3-VL 的性能通过一系列具有挑战性的基准进行了严格评估：ERQA（Team 等，2025）、VSIBench（Yang 等，2025b）、EmbSpatial（Du 等，2024）、RefSpatial（Zhou 等，2025）和 RoboSpatialHome（Song 等，2025a）。在这些基准上，该模型展现了卓越的能力，与顶级模型如 Gemini-2.5-Pro、GPT-5 和 Claude-Opus-4.1 相媲美。这一成功主要归功于模型深刻的空间理解能力，这源于其在高分辨率视觉数据上的训练，包括细粒度指向、相对位置注释和问答对。这一能力在 EmbSpatial、RefSpatial 和 RoboSpatialHome 上的强表现得到了明确验证，Qwen3-VL-235B-A22 分别获得了 84.3、69.9 和 73.9 的分数。此外，通过在训练过程中集成指向、定位和时空感知数据，其身体智能得到了显著增强，导致在 ERQA（Team 等，2025）上获得 52.5 的顶级分数，在 VSIBench（Yang 等，2025b）上获得 60.0 的顶级分数。

2025b) 用于 Qwen3-VL-235B-A22B。
5.9 视频理解
得益于训练数据的扩展和关键架构的改进，Qwen3-VL 展现了显著提升的视频理解能力。特别是交织式 MRoPE 的集成、文本时间戳的插入以及时间密集型视频字幕的扩展，共同使 Qwen3-VL 8B 版本在性能上能够与显著更大的 Qwen2.5-VL 72B 模型相媲美。
我们在多种视频理解任务中进行了全面评估，包括通用视频理解（VideoMME (Fu et al., 2024a)，MVBench (Li et al., 2024b)），时间视频定位（Charades-STA (Gao et al., 2017)），视频推理（VideoMMMU (Hu et al., 2025)，MMVU (Zhao et al., 2025)），以及长视频理解（LVBench (Wang et al., 2024d)，MLVU (Zhou et al., 2024)）。与最先进的专有模型——包括 Gemini 2.5 Pro、GPT-5 和 Claude Opus 4.1 相比，Qwen3-VL 表现出竞争力，并在某些情况下表现出优越性。特别是我们的旗舰模型 Qwen3-VL-235B-A22B-Instruct，在标准视频理解基准测试中表现与领先的模型如 Gemini 2.5 Pro（思考预算为 128）和 GPT-5 minimal 相当。通过将上下文窗口扩展到 256K 个 token，它在长视频评估任务中进一步达到甚至超过了 Gemini-2.5-Pro，尤其是在 MLVU 上。
关于评估细节，我们对所有基准测试中的每个视频设定了 2,048 帧的上限，确保视频 token 的总数不超过 224K。每帧的最大 token 数量在 VideoMMMU 和 MMVU 中设置为 768，在其他所有基准测试中设置为 640。此外，Charades-STA 的视频以每秒 4 帧（fps）采样，而其他所有基准测试则使用每秒 2 帧的速率。对于 VideoMMMU，我们采用基于模型的评判方法进行评估，因为基于规则的评分不够准确。值得注意的是，由于资源和 API 限制，我们的比较无法保证完全公平，这些限制影响了评估过程中使用的输入帧数：Gemini 2.5 Pro 为 512 帧，GPT-5 为 256 帧，Claude Opus 4.1 为 100 帧。
5.10 代理
我们通过 GUI 定位任务（ScreenSpot (Cheng et al., 2024)，ScreenSpot Pro (Li et al., 2025b)，OSWorldG (Xie et al., 2025a)）评估 UI 感知，并通过在线环境评估（AndroidWorld (Rawles et al., 2024)，OSWorld (Xie et al., 2025c;b)）评估决策能力。在 GUI 定位方面，Qwen3-VL-235B-A22B 在多个任务中实现了最先进的性能，涵盖了桌面、移动和 PC 上的交互界面，展示了极其强大的 UI 感知能力。在在线评估中，Qwen3-VL 32B 在 OSWorld 中得分为 41，在 AndroidWorld 中得分为 63.7，超过了当前的基础 VLM 模型。作为 GUI 代理，Qwen3-VL 展示了极其强大的规划、决策和反思能力。此外，较小的 Qwen3-VL 模型在这类基准测试中也表现出高度竞争力。
5.11 以文本为中心的任务
为了全面评估 Qwen3-VL 的以文本为中心的性能，我们采用了自动基准测试来评估指令和思考模型的性能。这些基准测试可以分为以下主要类型：(1) 知识：MMLU-Pro (Wang et al., 2024f)，MMLU-Redux (Gema et al., 2024)，GPQA (Rein et al., 2023)，SuperGPQA (Team, 2025)，(2) 推理：AIME-25 (AIME, 2025)，HMMT-25 (HMMT, 2025)，LiveBench (2024-11-25) (White et al., 2024)，(3) 代码：LiveCodeBench v6 (Jain et al., 2024)，CFEval，OJBench (Wang et al., 2025c)，(4) 对齐任务：IFEval (Zhou et al., 2023)，Arena-Hard v2 (Li et al., 2024d)1，Creative Writing v3 (Paech, 2023)2，WritingBench (Wu et al., 2025b)，(5) 代理：BFCL-v3 (Patil et al., 2024)，TAU2-Retail，TAU2-Airline，TAU2-Telecom，(6) 多语言：MultiIF (He et al., 2024)，MMLU-ProX，INCLUDE (Romanou et al., 2025)，PolyMATH (Wang et al., 2025b)。
评估设置：对于 Qwen3-VL 指令模型，包括 235B-A22B、32B 和 30B-A3B，我们将采样超参数配置为温度 = 0.7，top-p = 0.8，top-k = 20，存在惩罚 = 1.5。对于较小的指令模型，包括 8B、4B 和 2B，我们将温度设置为 1.0，top-p = 1.0，top-k = 40，存在惩罚 = 2.0。我们将最大输出长度设置为 32,768 个 token。
对于具有混合专家（MoE）架构的 Qwen3-VL 思考模型，我们将采样温度设置为 0.6，top-p 设置为 0.95，top-k 设置为 20。对于密集型思考模型，我们将温度设置为 1.0，top-p
1 为了重现 Arena-Hard v2，我们报告了由 GPT-4.1 评估的胜率。
2 为了重现 Creative Writing v3，我们报告了由 Claude 3.7 Sonnet 评估的分数。
20

表5：Qwen3-VL-235B-A22B (Instruct) 与其他基线模型的比较。最高分和次高分分别以粗体和下划线标出。
基准Qwen3-VL
235B-A22B
指令Qwen3
235B-A22B
指令-2507Deepseek V3
0324Claude-Opus-4
（无思考）
知识MMLU-Pro 81.8 83.0 81.286.6
MMLU-Redux 92.2 93.1 90.494.2
GPQA 74.377.568.4 74.9
SuperGPQA 60.4 62.657.3 56.5
AIME-2574.770.3 46.6 33.9
推理 HMMT-2557.455.4 27.5 15.9
LiveBench2024-11-2574.8 75.466.9 74.6
对齐任务
IFEval 87.8 88.782.3 87.4
Arena-Hard V2（胜率）77.4 79.245.6 51.5
创意写作 v3 86.5 87.581.6 83.8
WritingBench85.585.2 74.5 79.2
编码与代理LiveCodeBench v654.351.8 45.2 44.6
BFCL-v3 67.7 70.964.7 60.1
多语言能力MultiIF 76.3 77.566.5 -
MMLU-ProX 77.8 79.475.8 -
INCLUDE 80.0 79.580.1-
PolyMATH 45.1 50.232.2 30.0
设置最大输出长度为32,768个token，除了AIME-25、HMMT-25和LiveCodeBench v6，这些任务的长度扩展到81,920个token，以提供足够的思考空间。此外，我们还应用了1.5的出现惩罚，以鼓励更大的输出多样性。详细结果如下。

Qwen3-VL-235B-A22B 我们将我们的旗舰模型Qwen3-VL-235B-A22B与领先的指令和思考模型进行比较。对于Qwen3-VL-235B-A22B-Instruct，我们将Qwen3-235B-A22B-Instruct-2507、DeepSeek V3 0324和Claude-Opus-4（无思考）作为基线。对于Qwen3-VL-235B-A22B-Thinking，我们将Qwen3-235B-A22B-Thinking-2507、OpenAI o3（中等）和Claude-Opus-4（有思考）作为基线。我们在表5和表6中展示了评估结果。
•从表5可以看出，Qwen3-VL-235B-A22B-Instruct取得了具有竞争力的结果，与包括DeepSeek V3 0324、Claude-Opus-4（无思考）和我们之前的旗舰模型Qwen3-235B-A22B-Instruct-2507在内的其他领先模型相当甚至超越。特别是，Qwen3-VL-235B-A22B-Instruct在需要推理的任务（例如数学和编码）上超过了其他模型。值得注意的是，DeepSeek V3 0324和Qwen3-235B-A22B-Instruct-2507是大型语言模型，而Qwen3-VL-235B-A22B-Instruct是一个视觉语言模型，可以处理视觉和文本任务。这意味着Qwen3-VL-235B-Instruct已经实现了视觉和文本能力的集成。
•从表6可以看出，Qwen3-VL-235B-A22B-Thinking在与其他领先的思考模型相比也取得了具有竞争力的结果。Qwen3-VL-235B-A22B-Thinking在AIME-25和LiveCodeBench v6上超过了OpenAI o3（中等）和Claude-Opus-4（有思考），这表明Qwen3-VL-235B-A22B-Thinking具有更强的推理能力。
Qwen3-VL-32B / 30B-A3B 我们将我们的Qwen3-VL-32B和Qwen3-VL-30B-A3B模型与其相应的纯文本模型进行比较，即Qwen3-32B、Qwen3-30B-A3B和Qwen3-30B-A3B-2507。我们在表7和表8中展示了评估结果。
•从表7可以看出，对于指令模型，Qwen3-VL-32B和Qwen3-VL-30B-A3B在所有基准测试中都显著优于Qwen3-32B和Qwen3-30B-A3B。Qwen3-VL-30B-A3B在AIME-25和HMMT-25上的表现与Qwen3-30B-A3B-2507相当甚至更好。
•从表8可以看出，对于思考模型，Qwen3-VL-32B和Qwen3-VL-30B-A3B在大多数基准测试中都超过了基线模型。Qwen3-VL-30B-A3B的表现也与Qwen3-30B-A3B-2507相当。
21

表6：Qwen3-VL-235B-A22B（思考）与其他推理基线的比较。最高分和第二高分分别以粗体和下划线显示。
基准 Qwen3-VL
235B-A22B
思考 Qwen3
235B-A22B
思考-2507 OpenAI o3
（中等） Claude-Opus-4
（带思考）
知识 MMLU-Pro 83.8 84.4 85.9 -
MMLU-Redux 93.7 93.8 94.9 94.6
GPQA 77.1 81.1 83.3（高） 79.6
SuperGPQA 64.3 64.9 - -
AIME-25 89.7 92.3 88.9（高） 75.5
推理 HMMT-25 77.4 83.9 77.5 58.3
LiveBench2024-11-25 79.6 78.4 78.3 78.2
LiveCodeBench v6 70.1 74.1 58.6 48.9
编程 CFEval 1964 2134 2043 -
OJBench 27.5 32.5 25.4 -
对齐任务 IFEval 88.2 87.8 92.1 89.7
Arena-Hard V2（胜率） 74.8 79.7 80.8 59.1
创意写作 v3 85.7 86.1 87.7 83.8
WritingBench 86.7 88.3 85.3 79.1
代理 BFCL-v3 71.8 71.9 72.4 61.8
TAU2-Retail 67.0 71.9 76.3 -
TAU2-Airline 62.0 58.0 70.0 -
TAU2-Telecom 44.7 45.6 60.5 -
多语言 MultiIF 79.1 80.6 80.3 -
MMLU-ProX 80.6 81.0 83.3 -
INCLUDE 80.0 81.0 86.6 -
PolyMATH 57.8 60.1 49.7 -

表7：Qwen3-VL-32B-Instruct、Qwen3-VL-30B-A3B-Instruct及其相应基线的比较。
基准 Qwen3-VL
32B
指令 Qwen3
32B
指令 Qwen3-VL
30B-A3B
指令 Qwen3
30B-A3B
指令 Qwen3
30B-A3B
指令-2507
知识 MMLU-Pro 78.6 71.9 77.8 69.1 78.4
MMLU-Redux 89.8 85.7 88.4 84.1 89.3
GPQA 68.9 54.6 70.4 54.8 70.4
SuperGPQA 54.6 43.2 53.1 42.2 53.4
推理 AIME-25 66.2 20.2 69.3 21.6 61.3
HMMT-25 46.1 10.9 50.6 12.0 43.0
LiveBench2024-11-25 72.2 31.3 65.4 59.4 69.0
对齐任务 IFEval 84.7 83.2 85.8 83.7 84.7
Arena-Hard V2（胜率） 64.7 37.4 58.5 24.8 69.0
创意写作 v3 85.6 80.6 84.6 68.1 86.0
WritingBench 82.9 81.3 82.6 72.2 85.5
编程与代理 LiveCodeBench v6 43.8 29.1 42.6 29.0 43.2
BFCL-v3 70.2 63.0 66.3 58.6 65.1
多语言 MultiIF 72.0 70.7 66.1 70.8 67.9
MMLU-ProX 73.4 69.3 70.9 65.1 72.0
INCLUDE 74.0 69.6 71.6 67.8 71.9
PolyMATH 40.5 22.5 44.3 23.3 43.1

Qwen3-VL-8B / 4B / 2B 我们在表9和表10中展示了Qwen3-VL-2B、Qwen3-VL-4B和Qwen3-VL-8B的评估结果。对于Qwen3-VL-2B和Qwen3-VL-8B，我们将其与Qwen3-1.7B和Qwen3-8B进行比较。对于Qwen3-VL-4B，我们将其与Qwen3-4B和Qwen3-4B-2507进行比较。总体而言，这些边缘侧模型表现出色，优于基线。这些结果表明
22

表8：Qwen3-VL-32B (思考)、Qwen3-VL-30B-A3B (思考) 及其对应基线的比较。
基准Qwen3-VL
32B
思考Qwen3
32B
思考Qwen3-VL
30B-A3B
思考Qwen3
30B-A3B
思考Qwen3
30B-A3B
思考-2507
知识MMLU-Pro 82.1 79.1 80.5 78.5 80.9
MMLU-Redux 91.9 90.9 90.9 89.5 91.4
GPQA 73.1 68.4 74.4 65.8 73.4
SuperGPQA 59.0 54.1 56.4 51.8 56.8
推理AIME-25 83.7 72.9 83.1 70.9 85.0
HMMT-25 64.6 51.8 67.6 49.8 71.4
LiveBench2024-11-2574.7 65.7 72.1 74.3 76.8
编码LiveCodeBench v6 65.6 60.6 64.2 57.4 66.0
CFEval 1842 1986 1894 1940 2044
OJBench 20.0 24.1 23.4 20.7 25.1
对齐
任务IFEval 87.8 85.0 81.7 86.5 88.9
Arena-Hard V2(胜率)60.5 50.3 56.7 36.3 56.0
创意写作 v3 83.3 84.4 82.5 79.1 84.4
WritingBench 86.2 78.4 85.2 77.0 85.0
代理BFCL-v3 71.7 70.3 68.6 69.1 72.4
TAU2-Retail 59.4 59.6 64.0 34.2 58.8
TAU2-Airline 52.5 38.0 48.0 36.0 58.0
TAU2-Telecom 46.9 26.3 27.2 22.8 26.3
多语言能力MultiIF 78.0 73.0 73.0 72.2 76.4
MMLU-ProX 77.2 74.6 76.1 73.1 76.4
INCLUDE 76.3 73.7 74.5 71.9 74.4
PolyMATH 52.0 47.4 51.7 46.1 52.6
表9：Qwen3-VL-2B (指令)、Qwen3-VL-4B (指令)、Qwen3-VL-8B (指令) 及其对应基线的比较。
基准Qwen3-VL
2B
指令Qwen3-VL
4B
指令Qwen3-VL
8B
指令Qwen3
1.7B
指令Qwen3
4B
指令Qwen3
8B
指令Qwen3
4B
指令-2507
知识MMLU-Pro 49.0 67.1 71.6 42.3 58.0 63.4 69.6
MMLU-Redux 66.5 81.5 84.9 63.6 77.3 79.5 84.2
GPQA 42.0 55.9 61.9 34.7 41.7 39.3 62.0
SuperGPQA 24.3 40.3 44.5 22.8 32.0 35.8 42.8
推理AIME-25 22.2 46.6 45.9 10.6 19.1 20.9 47.4
HMMT-25 10.9 30.7 32.5 6.2 12.1 11.8 31.0
LiveBench2024-11-2539.5 60.9 62.0 35.6 48.4 53.5 63.0
对齐
任务IFEval 68.2 82.3 83.7 67.1 81.2 83.0 83.4
Arena-Hard V2(胜率)6.4 30.4 46.3 4.1 9.5 15.5 43.4
创意写作 v3 48.6 72.3 77.0 49.1 53.6 69.0 83.5
WritingBench 73.0 82.5 83.1 65.1 68.5 71.4 83.4
编码与代理LiveCodeBench v6 20.3 37.9 39.3 16.1 26.4 25.5 35.1
BFCL-v3 55.4 63.3 66.3 52.2 57.6 60.2 61.9
多语言能力MultiIF 43.2 61.5 66.8 43.2 61.3 69.2 69.0
MMLU-ProX 38.8 59.4 65.4 33.5 49.6 58.0 61.6
INCLUDE 45.8 61.4 67.0 42.6 53.8 62.5 60.1
PolyMATH 14.9 28.8 30.4 10.3 16.6 18.8 31.1
我们的强到弱蒸馏方法的有效性，使得我们能够以显著减少的成本和努力构建轻量级模型。
23

表10：Qwen3-VL-2B (思考)，Qwen3-VL-4B (思考)，Qwen3-VL-8B (思考) 与相应基线的比较。
基准 Qwen3-VL 2B 思考 Qwen3-VL 4B 思考 Qwen3-VL 8B 思考 Qwen3 1.7B 思考 Qwen3 4B 思考 Qwen3 8B 思考 Qwen3 4B 思考-2507
知识 MMLU-Pro 62.3 73.6 77.3 58.1 70.4 74.6 74.0
MMLU-Redux 76.9 86.0 88.8 73.9 83.7 87.5 86.1
GPQA 49.5 64.1 69.9 27.9 55.9 62.0 65.8
SuperGPQA 34.6 46.8 51.2 31.2 42.7 47.6 47.8
推理 AIME-25 39.0 74.5 80.3 36.8 65.6 67.3 81.3
HMMT-25 22.8 53.1 60.6 24.3 42.1 43.2 55.5
LiveBench2024-11-25 50.1 68.4 69.8 51.1 63.6 67.1 71.8
对齐任务 IFEval 75.1 82.6 83.2 72.5 81.9 85.0 87.4
Arena-Hard V2 (胜率) 12.0 36.8 51.1 4.7 13.7 29.1 34.9
创意写作 v3 55.6 76.1 82.4 50.6 61.1 78.5 75.6
写作基准 77.9 84.0 85.5 68.9 73.5 75.0 83.3
编码与代理 LiveCodeBench v6 29.3 51.3 58.6 31.3 48.4 51.0 55.2
BFCL-v3 57.2 67.3 63.0 56.6 65.9 68.1 71.2
多语言能力 MultiIF 58.9 73.6 75.1 51.2 66.3 71.2 77.3
MMLU-ProX 55.1 65.0 70.7 50.4 61.0 68.1 64.2
INCLUDE 53.3 64.6 69.5 51.8 61.8 67.8 64.4
PolyMATH 28.0 44.6 47.5 25.2 40.0 42.7 46.2

5.12 消融研究
5.12.1 视觉编码器
我们进行了与原始 SigLIP-2 的对比实验。如表11所示，在CLIP预训练阶段的零样本评估中，Qwen3-ViT 在标准基准上保持了竞争力，同时在 OmniBench 上取得了显著的性能提升，OmniBench 是我们设计的一套全面评估世界知识整合能力的综合测试套件，涵盖多样且具有挑战性的条件。此外，当与相同的 1.7B Qwen3 语言模型集成并训练 1.5T 个 token 后，Qwen3-ViT 在多个关键任务上始终优于基于 SigLIP-2 的基线，并在 OmniBench 上保持显著优势，证明了其作为更强视觉骨干的有效性和优越性。
表11：Qwen3-ViT 的消融研究。我们在 CLIP 预训练阶段比较了 Qwen3-ViT 和 SigLIP-2 的性能指标，并进一步评估了它们在视觉-语言建模 (VLM) 阶段与相同 1.7B Qwen3 语言模型配对后的下游性能。
ViT Clip 基准 VLM 基准
ImageNet-1K ImageNet-V2 ImageNet-A ImageNet-R ImageNet-S ObjectNet Omni OCRB AI2D RLWDQA InfoVQA Omni
SigLIP-2 84.2 78.6 87.0 96.1 76.2 79.9 36.9 77.2 74.1 58.7 65.3 50.1
Qwen3-ViT 84.6 78.8 87.1 95.7 74.5 81.0 45.5 78.7 76.2 66.1 67.0 53.0

5.12.2 DeepStack
我们进行了一项消融研究以验证 DeepStack 机制的有效性。如表12所示，配备 DeepStack 的模型在各种基准上实现了整体性能提升，强烈证实了其有效性。这一提升归因于 DeepStack 能够整合丰富的视觉信息，有效提升了细粒度视觉理解的能力，例如在 InfoVQA 和 DocVQA 基准上的表现。
表12：DeepStack 的消融研究。我们使用内部 15B-A2B 大型语言模型 (LLM) 对 DeepStack 进行了消融研究，所有实验均在 2000 亿个 token 上进行了预训练。我们直接在验证集上评估这些预训练模型，未进行任何后训练。
方法 AVG AI2D OCRB TVQA InfoVQA ChartQA DocVQA MMMU MMStar RLWDQA MMB ENMMB CN
基线 74.7 81.8 81.0 80.6 71.9 81.5 89.5 52.9 55.5 67.7 81.0 78.1
DeepStack 76.0 83.2 83.6 80.5 74.2 83.3 91.1 54.1 57.7 68.1 81.2 78.5
24

5分钟 10分钟 15分钟
(128k)20分钟 25分钟 30分钟
(256k)0%
20%
40%
60%
80%
100%深度（%）
0在训练上下文中（0-30分钟）
40分钟 50分钟 60分钟
(512k)70分钟 80分钟 90分钟
(768k)100分钟 110分钟 120分钟
(1024k)外推上下文（40-120分钟）
0.00.20.40.60.81.0
准确率得分
上下文长度图3：Qwen3-VL-235B-A22B-Instruct 在不同视频时长和针位置下的 Needle-in-a-Haystack 性能热图。每个单元格显示定位和回答插入的“针”帧问题的准确率（%）。
5.12.3 Needle-in-a-Haystack
为了评估模型处理长上下文输入的能力，我们在 Qwen3-VL-235B-A22B-Instruct 上构建了一个视频“Needle-in-a-Haystack”评估任务。在这个任务中，一个语义显著的“针”帧——包含关键视觉证据——被插入到长视频的不同时间位置。然后，模型需要准确地从长视频中定位目标帧并回答相应的问题。在评估过程中，视频以1 FPS均匀采样，帧分辨率动态调整以保持恒定的视觉令牌预算。
如图3所示，该模型在长达30分钟的视频上实现了完美的100%准确率——对应的上下文长度为256K令牌。值得注意的是，即使通过基于YaRN的位置扩展将序列扩展到高达1M令牌（大约2小时的视频），模型仍保持99.5%的高准确率。这些结果强烈证明了模型强大的长序列建模能力。
6 结论
在这项工作中，我们介绍了 Qwen3-VL，这是一系列最先进的视觉-语言基础模型，推动了多模态理解和生成的前沿。通过整合高质量的多模态数据迭代和架构创新——如增强的交错MRoPE、DeepStack视觉-语言对齐和基于文本的时间接地——Qwen3-VL 在广泛的多模态基准测试中实现了前所未有的性能，同时保持了强大的纯文本能力。其对256K令牌交错序列的原生支持使其能够对长而复杂的文档、图像序列和视频进行稳健推理，使其特别适合需要高保真跨模态理解的实际应用。密集型和专家混合型变体的可用性确保了在不同的延迟和质量要求下灵活部署，我们的后训练策略包括非思考模式和思考模式。
展望未来，我们设想 Qwen3-VL 作为能够无缝连接数字世界和物理世界的具身AI代理的基础引擎。这样的代理不仅能够感知和推理丰富的多模态输入，还能够在动态环境中执行决定性的、上下文感知的动作——与用户互动、操作数字界面，并通过基于地面的多模态决策引导机器人系统。未来的工作将集中在扩展 Qwen3-VL 的能力，朝着交互式感知、工具增强推理和实时多模态控制的方向发展，最终目标是实现能够与人类在虚拟和物理领域共同学习、适应和协作的AI系统。此外，我们正在积极研究统一的理解-生成架构，利用视觉生成能力进一步提升整体智能。通过在Apache 2.0许可下公开发布整个模型家族，我们旨在推动社区驱动的创新，朝着真正集成的多模态AI代理的愿景迈进。
25

7 贡献者与致谢
Qwen3-VL 的所有贡献者按姓氏字母顺序列出。
核心贡献者：白帅，蔡宇轩，陈睿哲，陈克勤，陈雄辉，程泽森，邓良浩，丁伟，高畅，葛春江，葛文斌，郭志芳，黄启东，黄杰，黄飞，惠斌元，姜姝彤，李兆海，李明盛，李梅，李凯欣，林子成，林俊阳，刘雪晶，刘佳伟，刘成龙，刘洋，刘大一恒，刘世轩，吕晨旭，罗瑞霖，吕晨旭，门睿，孟令晨，任轩成，任兴璋，宋思博，孙玉冲，唐军，涂建宏，万建强，王鹏，王鹏飞，王秋月，王晓萱，谢天宝，徐义恒，徐海洋，许进，杨志波，杨明坤，杨建新，杨安，余博文，张飞，张航，张曦，郑波，钟虎民，周静仁，周帆，周静，朱元智，朱科
贡献者：曹一中，陈贝，程晨，储云飞，崔泽宇，当凯，邓晓东，樊阳，方荣耀，管同坤，何金正，洪淼，蒋松涛，李峥，李晓川，林俊榕，刘玉琼，刘艳涛，倪娜，牛心瑶，庞亚天，邱子涵，沈天浩，唐天一，万宇，魏金西，吴晨飞，吴步潇，徐潇，薛明锋，颜明，杨雨环，杨嘉熙，杨可欣，于乐，于浩，张建科，张建伟，张一昌，张振儒，张思琪，张培洋，张北辰，赵洪波，庄显伟
致谢：我们衷心感谢由陈祖龙、邓兵、高飞宇、姜冠军、刘悦、邢航迪和余大君领导的团队提供的坚定支持。
参考文献
Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024.
AIME. Aime problems and solutions, 2025. URL https://artofproblemsolving.com/wiki/index.php/AIMEProblemsandSolutions.
Anthropic. Claude opus 4.1, 2025. URL https://www.anthropic.com/news/claude-opus-4-1.
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025.
Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: A diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021.
Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia Gkioxari. Omni3d: A large benchmark and model for 3d object detection in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 13154–13164, 2023.
Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv:2403.20330, 2024a.
Shimin Chen, Xiaohan Lan, Yitian Yuan, Zequn Jie, and Lin Ma. Timemarker: A versatile video-llm for long and short video understanding with superior temporal localization ability. arXiv preprint arXiv:2411.18211, 2024b.
Yitong Chen, Lingchen Meng, Wujian Peng, Zuxuan Wu, and Yu-Gang Jiang. Comp: Continual multi-modal pre-training for vision foundation models. arXiv preprint arXiv:2503.18931, 2025.
Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024.
Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, et al. Simplevqa: Multimodal factuality evaluation for multimodal large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4637–4646, 2025.
26

Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, 等. Gemini 2.5: 推动前沿的高级推理、多模态、长上下文和下一代代理能力.arXiv 预印本 arXiv:2507.06261, 2025.

Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, 等. Molmo 和 Pixmo: 开源权重和数据以实现最先进的多模态模型.arXiv 预印本 arXiv:2409.17146, 2024.

Shizhe Diao, Yu Yang, Yonggan Fu, Xin Dong, Dan Su, Markus Kliegl, Zijia Chen, Peter Belcak, Yoshi Suhara, Hongxu Yin, 等. CLIMB: 基于聚类的迭代数据混合引导用于语言模型预训练.arXiv 预印本 arXiv:2504.13161, 2025.

Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, 和 Hervé Jégou. Faiss 库. 2024.

Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, 和 Zhongyu Wei. EmbSpatial-Bench: 使用大型视觉-语言模型对具身任务的空间理解进行基准测试.arXiv 预印本 arXiv:2406.05756, 2024.

Chengqi Duan, Kaiyue Sun, Rongyao Fang, Manyuan Zhang, Yan Feng, Ying Luo, Yufang Liu, Ke Wang, Peng Pei, Xunliang Cai, 等. CodePlot-COT: 通过代码驱动的图像进行数学视觉推理.arXiv 预印本 arXiv:2510.11718, 2025.

Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, 等. Video-MME: 视频分析中多模态大语言模型的首个全面评估基准.arXiv:2405.21075, 2024a.

Ling Fu, Biao Yang, Zhebin Kuang, Jiajun Song, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, Mingxin Huang, Zhang Li, Guozhi Tang, Bin Shan, Chunhui Lin, Qi Liu, Binghong Wu, Hao Feng, Hao Liu, Can Huang, Jingqun Tang, Wei Chen, Lianwen Jin, Yuliang Liu, 和 Xiang Bai. OCRBench v2: 用于评估大型多模态模型在视觉文本定位和推理上的改进基准，2024b. URL https://arxiv.org/abs/2501.00321.

Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, 和 Ranjay Krishna. BLINK: 多模态大语言模型可以看见但不能感知. 在 European Conference on Computer Vision, 第 148–166 页. Springer, 2024c.

Chang Gao, Chujie Zheng, Xiong-Hui Chen, Kai Dang, Shixuan Liu, Bowen Yu, An Yang, Shuai Bai, Jingren Zhou, 和 Junyang Lin. 软自适应策略优化.arXiv 预印本 arXiv:2511.20347, 2025.

Jiyang Gao, Chen Sun, Zhenheng Yang, 和 Ram Nevatia. TALL: 通过语言查询进行时间活动定位. 在 Proceedings of the IEEE International Conference on Computer Vision, 第 5267–5275 页, 2017.

Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, 和 Pasquale Minervini. 我们是否已经完成 MMLU？CoRR, abs/2406.04127, 2024. doi: 10.48550/ARXIV.2406.04127. URL https://doi.org/10.48550/arXiv.2406.04127.

Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, 和 Tianyi Zhou. HallusionBench: 用于大型视觉-语言模型中纠缠语言幻觉与视觉幻觉的高级诊断套件，2023.

Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, Shruti Bhosale, Chenguang Zhu, Karthik Abinav Sankararaman, Eryk Helenowski, Melanie Kambadur, Aditya Tayade, Hao Ma, Han Fang, 和 Sinong Wang. Multi-IF: 对多轮和多语言指令跟随的大语言模型进行基准测试.CoRR, abs/2410.15553, 2024. doi: 10.48550/ARXIV.2410.15553. URL https://doi.org/10.48550/arXiv.2410.15553.

HMMT. HMMT 2025. https://www.hmmt.org, 2025.

Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, 和 Ziwei Liu. Video-MMU: 评估从多学科专业视频中获取知识.arXiv 预印本 arXiv:2501.13826, 2025.

27

黄杰，刘雪静，宋思博，侯瑞冰，常虹，林俊阳，白帅. 重新审视视觉-语言模型中的多模态位置编码，2025.
Naman Jain, King Han, Alex Gu, 李文鼎，严凡嘉，张天军，王思达，Armando Solar-Lezama，Koushik Sen，Ion Stoica. Livecodebench: 大型语言模型在代码评估中的整体性和无污染方法. CoRR, abs/2403.07974, 2024. doi: 10.48550/ARXIV .2403.07974. URL https://doi.org/10.48550/arXiv.2403.07974.
金博文，曾涵诗，岳振瑞，尹锦松，阿里克·塞坎，王东，扎马尼·哈梅德，韩家炜. Search-r1: 利用强化学习训练大型语言模型进行推理和利用搜索引擎. arXiv preprint arXiv:2503.09516, 2025.
Jeff Johnson, Matthijs Douze, Hervé Jégou. 十亿规模相似性搜索与GPU. IEEE Transactions on Big Data, 7(3):535–547, 2019.
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg. Referitgame: 指代自然场景照片中的对象. In EMNLP, 2014.
Aniruddha Kembhavi, Michael Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi. 一张图表胜过十二张图片. ArXiv, abs/1603.07396, 2016.
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, 等. 开放图像数据集v4: 统一的大规模图像分类、目标检测和视觉关系检测. International journal of computer vision, pp. 1956–1981, 2020.
赖欣，李俊一，李伟，刘涛，李天健，赵恒爽. Mini-o3: 扩展视觉搜索中的推理模式和交互轮次. arXiv preprint arXiv:2509.07969, 2025.
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, 等. Obelics: 一个开放的网页规模过滤数据集，包含交错的图文文档. Advances in Neural Information Processing Systems, 36:71683–71702, 2023.
李金科，余嘉睿，魏晨星，董涵，林强，杨良景，王志才，郝艳斌. UniSVG: 利用多模态大型语言模型进行矢量图形理解和生成的统一数据集. In Proceedings of the 33rd ACM International Conference on Multimedia, pp. 13156–13163, 2025a.
李凯欣，田宇辰，胡启盛，罗子阳，黄志勇，马静. MMCode: 通过视觉丰富的编程问题基准测试多模态大型语言模型的代码生成能力. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 736–783, 2024a.
李凯欣，孟子阳，林洪战，罗子阳，田宇辰，马静，黄志勇，蔡天盛. ScreenSpot-Pro: 高分辨率计算机专业使用中的GUI定位，2025b. URL https://likaixin2000.github.io/papers/ScreenSpot_Pro.pdf. 预印本.
李凯欣等. IconStack, 2025c. URL https://huggingface.co/datasets/likaixin/IconStack-48M-Rendered-Train.
李坤昌，王亚丽，何一楠，李逸卓，王毅，刘毅，王尊，徐继兰，陈国，罗平，等. MVbench: 全面的多模态视频理解基准. In CVPR, 2024b.
李流年·哈罗德，张鹏川，张浩天，杨建伟，李春元，钟义武，王丽娟，袁路，张雷，黄劲能，等. 基于地面的语言-图像预训练. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10965–10975, 2022.
李庆云，陈哲，王维云，王文海，叶圣龙，金振江，陈冠洲，何一楠，高章伟，崔尔非，等. Omnicorpus: 一个十亿级图像与文本交错的统一多模态语料库. arXiv preprint arXiv:2406.08418, 2024c.
李天乐，蒋韦林，Frick Evan，Dunlap Lisa，吴天浩，朱邦华，Joseph E. Gonzalez，Ion Stoica. 从众包数据到高质量基准：Arena-Hard和BenchBuilder管道. CoRR, abs/2406.11939, 2024d. doi: 10.48550/ARXIV .2406.11939. URL https://doi.org/10.48550/arXiv.2406.11939.
林聪义，Michael Maire，Serge Belongie，James Hays，Pietro Perona，Deva Ramanan，Piotr Dollár，C Lawrence Zitnick. Microsoft COCO: 上下文中的常见物体. In ECCV, 2014.
28

刘世龙，曾昭阳，任天和，李峰，张浩，杨杰，李春月，杨建伟，苏航，朱俊娟，张磊. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv:2303.05499, 2023a.
刘元，段浩东，李博，张远汉，张松阳，赵王波，袁一可，王佳琪，何聪慧，刘子微，陈凯，林达华. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281, 2023b.
刘玉良，李章，黄明欣，杨彪，余文文，李春园，尹旭成，刘成林，金连文，白翔. OCRbench: on the hidden mystery of OCR in large multimodal models. *中国科学信息科学*，67(12)，2024年12月. ISSN 1869-1919. doi: 10.1007/s11432-024-4235-6. URL <http://dx.doi.org/10.1007/s11432-024-4235-6>.
卢敦杰，徐一恒，王俊丽，吴浩源，王心远，王泽坤，杨俊林，苏洪进，陈吉轩，陈俊达，毛宇晨，周敬仁，林俊阳，惠斌远，于涛. Videoagenttrek: Computer use pretraining from unlabeled videos, 2025. URL <https://arxiv.org/abs/2510.19488>.
吕潘，Hritik Bansal，Tony Xia，刘家诚，李春园，Hannaneh Hajishirzi，程浩，张凯伟，Michel Galley，高剑锋. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv 预印本 arXiv:2310.02255, 2023.
马玉波，臧雨航，陈亮宇，陈美琪，焦一竹，李新泽，吕心远，刘子宇，马燕，董晓怡等. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. *神经信息处理系统进展*，37:95963–96010, 2024.
毛俊华，Jonathan Huang，Alexander Toshev，Oana Camburu，Alan L Yuille，Kevin Murphy. Generation and comprehension of unambiguous object descriptions. 在 *CVPR*, 2016.
Ahmed Masry，Do Xuan Long，Tan Jiaqing，Shafiq Joty，Enamul Hoque. ChartQA: A benchmark for question answering about charts with visual and logical reasoning. arXiv:2203.10244, 2022.
Minesh Mathew，Viraj Bagal，Rubén Pérez Tito，Dimosthenis Karatzas，Ernest Valveny，C.V. Jawahar. InfographicVQA. 2022 IEEE/CVF 冬季计算机视觉应用会议 (WACV)，pp. 2582–2591, 2021a.
Minesh Mathew，Dimosthenis Karatzas，C.V. Jawahar. DocVQA: A dataset for VQA on document images. 在 *WACV*, 2021b.
孟令晨，杨建伟，田睿，戴西阳，吴祖轩，高剑锋，江玉刚. DeepStack: 深度堆叠视觉令牌在多模态语言模型中出奇地简单有效. 在 *神经信息处理系统进展*，卷 37，pp. 23464–23487, 2024.
OpenAI. GPT-5 系统卡，2025. URL <https://cdn.openai.com/gpt-5-system-card.pdf>.
欧阳麟科，曲元，周宏伟，朱佳伟，张瑞，林群书，王斌，赵志远，姜曼，赵小萌，石进，吴凡，储佩，刘明浩，李振祥，徐超，张博，石博天，涂忠颖，何聪慧. OmniDocBench: 基准化具有全面注释的多样化 PDF 文档解析，2024. URL <https://arxiv.org/abs/2412.07626>.
Samuel J. Paech. EQ-Bench: 大型语言模型的情感智能基准. *CoRR*, abs/2312.06281, 2023. doi: 10.48550/ARXIV.2312.06281. URL <https://doi.org/10.48550/arXiv.2312.06281>.
Roni Paiss，Ariel Ephrat，Omer Tov，Shiran Zada，Inbar Mosseri，Michal Irani，Tali Dekel. Teaching CLIP to count to ten. 在 *IEEE/CVF 国际计算机视觉会议论文集*, pp. 3170–3180, 2023.
Patil Shishir G., Mao Huanzhi, Ji Charlie Cheng-Jie, Yan Fanjia, Suresh Vishnu, Stoica Ion, Gonzalez Joseph E. The Berkeley Function Calling Leaderboard (BFCL): From tool use to agentic evaluation of large language models. 在 *神经信息处理系统进展*, 2024.
钱宇素，叶翰荣，Jean-Philippe Fauconnier，Peter Grasch，杨胤飞，甘哲. MIA-Bench: Towards better instruction following evaluation of multimodal LLMs. arXiv 预印本 arXiv:2407.01509, 2024.
乔润琦，谭秋娜，董冠婷，吴明辉，孙冲，宋晓帅，贡曲卓玛，雷尚林，韦哲，张妙轩等. We-Math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv 预印本 arXiv:2407.01284, 2024.
29

Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, 和 Anh Totti Nguyen. 视觉语言模型是盲目的：无法将详细的视觉特征转化为文字，2025. URL https://arxiv.org/abs/2407.06581.
Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, 等. Androidworld：一个动态的自主代理基准测试环境.arXiv:2405.14573, 2024.
David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, 和 Samuel R. Bowman. GPQA：一个研究生级别的谷歌证明问答基准.CoRR, abs/2311.12022, 2023. doi: 10.48550/ARXIV.2311.12022. URL https://doi.org/10.48550/arXiv.2311.12022.
Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, 等. Zerobench：一个对当代大型多模态模型来说不可能完成的视觉基准，2025. URL https://arxiv.org/abs/2502.09696.
Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, 和 Joshua M Susskind. Hypersim：一个用于整体室内场景理解的高真实感合成数据集. 在 Proceedings of the IEEE/CVF international conference on computer vision, pp. 10912–10922, 2021.
Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Imanol Schlag, 等. INCLUDE：使用区域知识评估多语言理解. 在 The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025.
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, 和 Jian Sun. Objects365：一个大规模、高质量的目标检测数据集. 在 Proceedings of the IEEE/CVF international conference on computer vision, pp. 8430–8439, 2019.
Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, 和 Diyi Yang. Design2code：自动化前端工程的多模态代码生成基准. 在 Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 3956–3974, 2025.
Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, 和 Stan Birchfield. Robospatial：教授2D和3D视觉语言模型空间理解以应用于机器人技术. 在 Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 15768–15780, 2025a.
Shuran Song, Samuel P Lichtenberg, 和 Jianxiong Xiao. SUN RGB-D：一个RGB-D场景理解基准套件. 在 Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 567–576, 2015.
Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, 和 Xiang Yue. VisualPuzzles：从领域知识中解耦多模态推理评估.arXiv preprint arXiv:2504.10342, 2025b. URL https://arxiv.org/abs/2504.10342.
Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, 等. Gemini Robotics：将AI带入物理世界.arXiv preprint arXiv:2503.20020, 2025.
M-A-P Team. SuperGPQA：跨285个研究生学科的LLM评估扩展.CoRR, abs/2502.14739, 2025. doi: 10.48550/ARXIV.2502.14739. URL https://doi.org/10.48550/arXiv.2502.14739.
Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, 等. SigLIP 2：具有改进语义理解、定位和密集特征的多语言视觉语言编码器.arXiv preprint arXiv:2502.14786, 2025.
Fei Wang, Xingyu Fu, James Y Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, 等. MuirBench：一个全面的鲁棒多图像理解基准.arXiv preprint arXiv:2406.09411, 2024a.
Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, 和 Hongsheng Li. 通过Math-Vision数据集测量多模态数学推理. Advances in Neural Information Processing Systems, 37:95095–95169, 2024b.
30

Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: 增强视觉语言模型对世界任意分辨率的感知能力.arXiv:2409.12191, 2024c.

Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: 极端长视频理解基准.arXiv preprint arXiv:2406.08035, 2024d.

Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, and Dacheng Tao. 分而治之与结合：多模态大语言模型中无需训练的高分辨率图像感知框架.arXiv preprint, 2024e. URLhttps://arxiv.org/abs/2408.15556.

Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, et al. Opencua: 计算机使用代理的开放基础.arXiv preprint arXiv:2508.09123, 2025a.

Yiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, Feitong Sun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, et al. Polymath: 在多语言环境中评估数学推理能力.CoRR, abs/2504.18428, 2025b. doi: 10.48550/ARXIV .2504.18428. URLhttps://doi.org/10.48550/arXiv.2504.18428.

Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, et al. MMLU-Pro: 更加健壮和具有挑战性的多任务语言理解基准.CoRR, abs/2406.01574, 2024f.

Zhexu Wang, Yiping Liu, Yejie Wang, Wenyang He, Bofei Gao, Muxi Diao, Yanxu Chen, Kelin Fu, Flood Sung, Zhilin Yang, Tianyu Liu, and Weiran Xu. Ojbench: 大语言模型的竞赛级代码基准.CoRR, abs/2506.16395, 2025c. doi: 10.48550/ARXIV .2506.16395. URLhttps://doi.org/10.48550/arXiv.2506.16395.

Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, and Danqi Chen. Charxiv: 多模态大语言模型在现实图表理解中的差距分析.arXiv preprint arXiv:2406.18521, 2024g.

Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, and Luca Soldaini. 组织网络：构建领域以增强预训练数据管理.arXiv preprint arXiv:2502.10341, 2025.

Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, et al. Livebench: 具有挑战性且无污染的大语言模型基准.CoRR, abs/2406.19314, 2024. doi: 10.48550/ARXIV .2406.19314. URLhttps://doi.org/10.48550/arXiv.2406.19314.

Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearch-r1: 激励多模态大语言模型进行搜索.arXiv preprint arXiv:2506.20670, 2025a.

Penghao Wu and Saining Xie. V*: 多模态大语言模型中的引导视觉搜索机制. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 13084–13094, June 2024.

Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, and Fei Huang. Writingbench: 生成写作的全面基准.CoRR, abs/2503.05244, 2025b. doi: 10.48550/ARXIV .2503.05244. URLhttps://doi.org/10.48550/arXiv.2503.05244.

xAI. Realworldqa: 现实空间理解基准. https://huggingface.co/datasets/xai-org/RealworldQA, 2024. 访问日期: 2025-04-26.

Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: 视觉场景中的多模态大语言模型逻辑推理基准.arXiv preprint arXiv:2407.04973, 2024.

Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. 通过用户界面分解和合成扩展计算机使用接地, 2025a. URLhttps://arxiv.org/abs/2505.13227.

31

谢天宝，袁梦琪，张丹阳，熊新庄，沈振楠，周子龙，王新远，陈延旭，邓家琦，陈俊达，王博文，吴浩元，陈吉轩，王军利，吕敦杰，胡浩，余涛。介绍 osworld-verified.xlang.ai，2025 年 7 月。URL https://xlang.ai/blog/osworld-verified。
谢天宝，张丹阳，陈吉轩，李晓川，赵思恒，曹瑞生 等。Osworld：在真实计算机环境中为开放式任务基准测试多模态代理。《神经信息处理系统进展》，37:52040–52094，2025c。
徐伟业，王佳豪，王伟云，陈哲，周文刚，杨爱军，卢乐威，李厚强，王晓华，朱熙舟 等。Visulogic：评估多模态大语言模型中视觉推理能力的基准，2025。URL https://arxiv.org/abs/2504.15279。
杨安，李安峰，杨宝松，张北辰，惠斌源，郑波，于博文 等。Qwen3 技术报告，2025a。
杨成，石楚凡，刘亚欣，水波，王俊杰，景漠涵，许林然，朱心宇，李思恒，张宇翔 等。Chartmimic：通过图表到代码生成评估 LMM 的跨模态推理能力。arXiv 预印本 arXiv:2406.09961，2024a。
杨继涵，杨树生，安贾莉·W·古普塔，Rilyn Han，李飞飞，谢赛宁。空间思考：多模态大语言模型如何看到、记住和回忆空间。《计算机视觉与模式识别会议论文集》，第 10632–10643 页，2025b。
杨志博，唐军，李兆海，王鹏飞，万建强，钟虎民，刘雪静，杨明坤，王鹏，白帅，金连文，林俊阳。CC-OCR：一个全面且具有挑战性的 OCR 基准，用于评估多模态大模型在识字方面的表现，2024b。URL https://arxiv.org/abs/2412.02210。
叶嘉波，张曦，徐海阳，刘浩伟，王俊阳，朱兆庆，郑子维 等。Mobile-agent-v3：GUI 自动化的基础代理。arXiv 预印本 arXiv:2508.15144，2025。
岳翔，倪元胜，张凯，郑天宇，刘若琪，张戈 等。MMMU：一个大规模多学科多模态理解和推理基准，用于专家 AGI。《IEEE/CVF 计算机视觉与模式识别会议论文集》，第 9556–9567 页，2024a。
岳翔，郑天宇，倪元胜，王玉波，张凯，童圣邦，孙宇轩，俞博涛，张戈，孙欢 等。MMMU-Pro：一个更强大的多学科多模态理解基准。arXiv 预印本 arXiv:2409.02813，2024b。
张润睿，江东志，张一驰，林浩坤，郭子宇，邱鹏硕，周傲军，陆潘，张凯伟，乔宇 等。Mathverse：你的多模态大模型是否真正看到了视觉数学问题中的图表？《欧洲计算机视觉会议论文集》，第 169–186 页。Springer，2024。
赵一伦，谢路晶，张浩伟，甘国，龙逸涛，胡志远，胡通言，陈伟元，李楚涵，宋俊阳，徐志健，王成业 等。MMVU：衡量专家级多学科视频理解，2025。URL https://arxiv.org/abs/2501.12380。
郑子维，Michael Yang，Jack Hong，赵晨潇，许国海，杨乐，沈超，余星。DeepEyes：通过强化学习激励“用图像思考”。arXiv 预印本 arXiv:2505.14362，2025。
周恩森，安景坤，池成，韩毅，戎善雨，张驰，王鹏伟，王中远，黄铁军，盛璐 等。RoboRefer：在机器人视觉-语言模型中实现带推理的空间指代。arXiv 预印本 arXiv:2506.04308，2025。
周杰夫，陆天健，米什拉斯瓦鲁普，布拉马西达尔塔，巴苏苏乔伊，栾义，周德尼，侯乐。大语言模型指令跟随评估。CoRR，abs/2311.07911，2023。doi: 10.48550/ARXIV.2311.07911。URL https://doi.org/10.48550/arXiv.2311.07911。
周俊杰，舒燕，赵波，吴博雅，肖世涛，杨曦，熊永平，张波，黄铁军，刘正。MLVU：一个多任务长视频理解的综合基准。arXiv 预印本 arXiv:2406.04264，2024。
朱万荣，Hessel 杰克，Awadalla 安纳斯，Gadre 萨米尔·伊扎克，Dodge 杰西，Fang 亚历克斯，Yu Youngjae，Schmidt 路德维希，William Yang Wang，Choi Yejin。多模态 C4：一个开放的、十亿规模的图像与文本交织的语料库。《神经信息处理系统进展》，36:8958–8974，2023。
邹成科，郭兴港，杨睿，张俊宇，胡斌，张欢。DynaMath：一个动态视觉基准，用于评估视觉语言模型的数学推理鲁棒性。arXiv 预印本 arXiv:2411.00836，2024。
32

A 基准测试
我们在多个不同能力的广泛公共基准测试中评估了 Qwen3-VL，包括多模态推理、通用视觉问答、主观体验与指令跟随、文档理解（包括 OCR）、2D/3D 视觉定位和计数、空间推理、视频理解、GUI 代理和以文本为中心的任务。以下是所使用的所有基准测试的详细列表。
• 多模态推理：我们在涵盖从数学和 STEM 到视觉推理和解谜任务的 12 个基准测试中评估模型：MMMU (Yue et al., 2024a)，MMMU-Pro (Yue et al., 2024b)，MathVision (Wang et al., 2024b)，MathVision-Wild photo，MathVista (Lu et al., 2023)，We-Math (Qiao et al., 2024)，MathVerse (Zhang et al., 2024)，DynaMath (Zou et al., 2024)，Math-VR (Duan et al., 2025)，LogicVista (Xiao et al., 2024)，VisualPuzzles (Song et al., 2025b)，VLM are Blind (Rahmanzadehgervi et al., 2025)，ZeroBench (Main/Subtasks) (Roberts et al., 2025)，VisuLogic (Xu et al., 2025)。
• 通用视觉问答：我们在 4 个通用 VQA 基准测试中评估模型：MMBench-V1.1 (Liu et al., 2023b)，RealWorldQA (xAI, 2024)，MMStar (Chen et al., 2024a)，SimpleVQA Cheng et al. (2025)。
• 主观体验和指令跟随：我们在 3 个基准测试中评估模型，涵盖主观体验和复杂指令跟随：HallusionBench (Guan et al., 2023)，MM-MT-Bench (Agrawal et al., 2024)，MIA-Bench (Qian et al., 2024)。
• 文档理解：我们对 Qwen3-VL 系列在多种 OCR 相关基准测试中的 OCR 和文档理解能力进行全面评估：DocVQA (Mathew et al., 2021b)，InfoVQA (Mathew et al., 2021a)，AI2D (Kembhavi et al., 2016)，ChartQA (Masry et al., 2022)，OCRBench (Liu et al., 2024)，OCRBench_v2 (Fu et al., 2024b)，CC-OCR (Yang et al., 2024b)，OmniDocBench (Ouyang et al., 2024)，CharXiv (Wang et al., 2024g)，MMLongBench-Doc (Ma et al., 2024)。
• 2D/3D 定位和空间理解：我们在 11 个基准测试中评估模型，包括 2D 定位、3D 定位和空间理解：RefCOCO/+/g (Kazemzadeh et al., 2014; Mao et al., 2016)，ODinW-13 (Li et al., 2022)，CountBench (Paiss et al., 2023)，ARKitScenes (Baruch et al., 2021)，Hypersim (Roberts et al., 2021)，SUN RGB-D (Song et al., 2015)，ERQA (Team et al., 2025)，VSIBench (Yang et al., 2025b)，EmbSpatial (Du et al., 2024)，RefSpatial (Zhou et al., 2025)，RoboSpatialHome (Song et al., 2025a)。
• 视频理解：我们使用七个基准测试来评估模型的视频理解能力：VideoMME (Fu et al., 2024a)，MVBench (Li et al., 2024b)，VideoMMMU (Hu et al., 2025)，MMVU (Zhao et al., 2025)，LVBench (Wang et al., 2024d)，MLVU (Zhou et al., 2024)，Charades-STA (Gao et al., 2017)。
• 编码：我们使用 Design2Code (Si et al., 2025)，ChartMimic (Yang et al., 2024a)，和 UniSVG (Li et al., 2025a) 基准测试评估模型的多模态编码能力，特别是在前端重建和 SVG 生成方面。
• GUI 代理：我们使用测试感知和决策能力的基准测试来评估 GUI 代理能力。对于感知，我们使用 ScreenSpot (Cheng et al., 2024)，ScreenSpot Pro (Li et al., 2025b)，和 OSWorldG (Xie et al., 2025a) 来衡量跨设备的 GUI 定位和界面布局理解。对于决策，我们使用 AndroidWorld (Rawles et al., 2024) 和 OSWorld (Xie et al., 2025c;b) 来评估在真实或模拟操作环境中的交互控制、规划和执行。
• 以文本为中心的任务：我们在广泛的以文本为中心的数据集中评估模型。(1) 知识：MMLU-Pro (Wang et al., 2024f)，MMLU-Redux (Gema et al., 2024)，GPQA (Rein et al., 2023)，SuperGPQA (Team, 2025)；(2) 推理：AIME-25 (AIME, 2025)，HMMT-25 (HMMT, 2025)，LiveBench (2024-11-25) (White et al., 2024)；(3) 代码：LiveCodeBench v6 (Jain et al., 2024)，CFEval，OJBench (Wang et al., 2025c)；(4) 对齐任务：IFEval (Zhou et al., 2023)，Arena-Hard v2 (Li et al., 2024d)，Creative Writing v3 (Paech, 2023)，WritingBench (Wu et al., 2025b)；(5) 代理：BFCL-v3 (Patil et al., 2024)，TAU2-Retail，TAU2-Airline，TAU2-Telecom；(6) 多语言：MultiIF (He et al., 2024)，MMLU-ProX，INCLUDE (Romanou et al., 2025)，PolyMATH (Wang et al., 2025b)。
33

B 评估提示
为了确保可重复性并促进未来的研究，我们在此提供了用于评估我们模型的所有基准测试的完整提示集。这些提示在推理过程中始终应用，以保持公平性和可比性。
B.1 STEM 与谜题
MMMU
<image>
问题：{question}
选项：
{options}
请选择上述选项中的正确答案。
MMMUPro_Standard
<image>
{question}
{options}
请选择选项中的正确答案。
MMMUPro_Vision
<image>
识别问题并解决它。回答前请逐步思考。
MathVista | MathVision | MathVerse | LogicVista
<image>
{question}
We-Math
<image>
现在，我们需要您解决一个选择题数学问题。请简要描述您的思考过程并提供最终答案（选项）。
问题：{question}
选项：{options}
关于格式，请按照以下模板回答，并确保包含两个 <> 符号：
<思考过程>: «您的思考过程» <答案>: «您的选项»
ZeroBench
<image>
{question}
让我们逐步思考并给出最终答案，格式如下：{最终答案}
34

DynaMath
<image>
## 问题
{question}
## 回答指示：请对上述问题提供答案。您的回答应遵循以下 JSON 格式，包括两个键：’solution’ 和 ’short answer’。’solution’ 键可以包含解决问题所需的详细步骤，而 ’short answer’ 键应提供简洁的回答。
预期的 JSON 回答格式示例：
{
"solution": "[详细的逐步解释]",
"short answer": "[简洁答案]"
}
VLMBlind
<image>
问题：{question}
VisuLogic
<image>
{question}
通过逐步推理解决复杂的视觉逻辑推理问题。
首先思考推理过程，然后按照以下格式回答问题：
答案://boxed{$LETTER}
VisualPuzzles-Direct
<image>
问题：{question}
选项：
{options}
直接用给定选项的字母回答问题。
VisualPuzzles-CoT
<image>
问题：{question}
选项：
{options}
解决选择题，然后用给定选项的字母回答。您的回答的最后一行应为以下格式：’答案: $LETTER’（不带引号），其中 LETTER 是其中一个选项。回答前请逐步思考。
B.2 通用VQA
MMBench | RealWorldQA | MMStar
<image>
问题：{question}
选项：
{options}
请选择上述选项中的正确答案。
SimpleVQA
<image>
{question}
35

B.3 对齐
HallusionBench | MM_MT_Bench | MIA-Bench
<image>
{问题}
B.4 文档理解
MMLongBench-Doc
<image_1>
<image_2>
...
<image_n>
{问题}
DocVQA | InfoVQA | ChartQA_TEST
<image>
{问题}
用一个单词或短语回答问题。
AI2D
<image>
问题：{问题}
选项：
{选项}
请选择上述选项中的正确答案。
OCRBench | OCRBench_v2 | CC-OCR | CharXiv
<image>
{问题}
OmniDocBench
<image>
您是一个专门将PDF图像转换为Markdown格式的AI助手。请遵循以下转换说明：
1. 文本处理： - 准确识别PDF图像中的所有文本内容，不得猜测或推断。 - 将识别的文本转换为Markdown格式。 - 保持原始文档结构，包括标题、段落、列表等。
2. 数学公式处理： - 将所有数学公式转换为LaTeX格式。 - 用\( 包围内联公式。例如：这是一个内联公式 \( E = mc^2 \) - 用\[ 包围块公式。例如：\[ \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} \]
3. 表格处理： - 将表格转换为HTML格式。 - 用<table> 和 </table> 包围整个表格。
4. 图像处理： - 忽略PDF图像中的图像。不要尝试描述或转换图像。
5. 输出格式： - 确保输出的Markdown文档具有清晰的结构，并在元素之间适当换行。 - 对于复杂的布局，尽量保持原始文档的结构和格式。
请严格遵循这些指南，以确保转换的准确性和一致性。
您的任务是准确地将PDF图像的内容转换为Markdown格式，不添加任何额外的解释或评论。
36

B.5 2D/3D 定位
RefCOCO
<image>
在图像中定位与描述“{ref_sentence}”匹配的所有对象。报告bbox坐标，格式为JSON。
CountBench
<image>
问题：{问题}
选项：
{选项}
请选择上述选项中的正确答案。
ODinW-13
<image>
定位属于以下类别的所有实例：´{obj_names}´。报告bbox坐标，格式为JSON。
ARKitScenes | Hypersim | SUNRGBD
<image>
在提供的图像中定位{class_name}，并使用3D边界框输出其位置和尺寸。结果必须为JSON格式：["bbox_3d":[x_center, y_center, z_center, x_size, y_size, z_size, roll, pitch, yaw],"label":"category"]。
B.6 身体/空间理解
ERQA
<image_1>
<image_2>
...
<image_n>
{问题}
VSI-Bench
选择题：
<video>
这些是视频的帧。
{问题}
选项：
{选项}
直接用给定选项的字母作答。
开放式问题：
<video>
这些是视频的帧。
{问题}
请用一个单词或短语回答问题。
EmbSpatialBench
<image>
{问题}
37

RoboSpatialHome
<image>
在图像中定位 {object_name}。以 JSON 格式输出点坐标。
例如：
[
{"point_2d": [x, y], "label": "point_1"}
]

RefSpatialBench
<image>
{question} 以 JSON 格式输出点坐标。
例如：
[
{"point_2d": [x, y], "label": "point_1"}
]

B.9 工具辅助感知
V*
你的角色是专门从事视觉信息的研究助理。通过仔细观察图像并使用研究工具来回答有关图像的问题。请遵循这一结构化的思考过程，并展示你的工作。
对于每个问题，开始一个迭代循环：
- **首先，仔细观察：** 从对图像的详细描述开始，注意用户的问题。列出你仅凭观察就能知道的内容，以及你需要查找的内容。
- **接下来，查找信息：** 使用工具来研究你需要了解的事情。
- **然后，审查发现：** 仔细分析工具告诉你的信息，并决定下一步行动。
继续这个循环，直到你的研究完成。
最后，将所有内容整合成一个清晰、综合的答案，全面回应用户的问题。
#工具
你可以调用一个或多个函数来协助处理用户查询。
在 <tools></tools> XML 标签内提供了函数签名：
<tools>
{ "type":"function", "function": {"name": "image_zoom_in_tool", "description": "通过基于边界框（bbox）裁剪特定区域来放大图像，并可选地提供对象标签", "arguments": {"type": "object", "properties": {"bbox_2d": {"type": "array", "items": {"type": "number"}, "minItems": 4, "maxItems": 4, "description": "要放大的区域的边界框，表示为 [x1, y1, x2, y2]，其中 (x1, y1) 是左上角，(x2, y2) 是右下角"}, "label": {"type": "string", "description": "指定边界框内的对象名称或标签"}, "img_idx": {"type": "number", "description": "放大的图像索引（从0开始）"}}, "required": ["bbox_2d", "label", "img_idx"]}}}
</tools>
每次调用函数时，返回一个包含函数名称和参数的 JSON 对象，放在

HRBench4K | HRBench8K
您的角色是专门研究视觉信息的研究助理。通过仔细观察图像并使用研究工具来回答关于图像的问题。请遵循以下结构化思考过程，并展示您的工作。
对于每个问题，开始一个迭代循环：
- **首先，仔细观察：** 从对图像的详细描述开始，注意用户的问题。列出您仅凭观察可以知道的内容以及需要查找的内容。
- **接下来，查找信息：** 使用工具来研究您需要了解的事情。
- **然后，审查发现：** 仔细分析工具告诉您的内容，并决定下一步行动。
继续这个循环，直到您的研究完成。
最后，将所有内容综合成一个清晰、综合的答案，完全回应用户的问题。
#工具
您可以调用一个或多个函数来协助处理用户查询。
您可以在 <tools></tools> XML 标签内获得函数签名：
<tools>
{ "type":"function", "function": {"name": "image_zoom_in_tool", "description": "根据边界框（bbox）裁剪图像以放大特定区域，并可选地提供对象标签", "arguments": {"type": "object", "properties": {"bbox_2d": {"type": "array", "items": {"type": "number"}, "minItems": 4, "maxItems": 4, "description": "要放大的区域的边界框，表示为 [x1, y1, x2, y2]，其中 (x1, y1) 是左上角，(x2, y2) 是右下角"}, "label": {"type": "string", "description": "指定边界框内的对象名称或标签"}, "img_idx": {"type": "number", "description": "放大的图像索引（从 0 开始）"}}, "required": ["bbox_2d", "label", "img_idx"]}}}
</tools>
对于每次函数调用，返回一个包含函数名称和参数的 JSON 对象，格式如下：
<tool_call>
{{"name": <function-name>, "arguments": <args-json-object>}}
</tool_call>
<image>
{question}
{options}
B.10 编码
Design2Code（生成）
<image>
您是一位专门从事 HTML 和 CSS 的专家网页开发者。用户将提供一个网页的截图。您需要返回一个使用 HTML 和 CSS 重现该网站的单个 HTML 文件。将所有 CSS 代码包含在 HTML 文件中。如果涉及任何图像，请使用 "rick.jpg" 作为占位符。网页上的某些图像是用蓝色矩形作为占位符的，这些也使用 "rick.jpg"。不要虚构对外部文件的任何依赖。您不需要包括用于动态交互的 JavaScript 脚本。注意元素的大小、文本、位置和颜色，以及整体布局。请以 HTML+CSS 文件的内容作为回应：
41

Design2Code (GPT-o4-mini 评估)
我将给你两张图片。第一张是参考图，第二张是通过代码渲染从第一张生成的。请从 0 到 100 对它们的相似度进行评分，其中 0 表示完全不同，100 表示完全相同。请在 LaTeX 中提供分数，并简要说明你的理由。
<reference_image>
<generated_image>
B.11 代理
Screenspot | Screenspot-Pro | OSWorld-G
工具
你可以调用一个或多个函数来协助用户查询。
提供的函数签名位于 <tools>...</tools> XML 标签内：
<tools>{"name":"computer_use","description": "使用鼠标与计算机交互。屏幕分辨率为 <display_width_px>x <display_height_px>。" "notes": "点击时将光标尖端对准目标中心；除非要求，否则避免边缘。不要使用其他工具（输入、键、滚动、左键拖动）。仅允许使用左键单击和鼠标移动。如果找不到元素，请终止并报告失败。",
"parameters":{ "type":"object", "required":["action"], "properties":{ "action":{ "type":"string", "enum":["mouse_move","left_click"], "description":"要执行的操作。" }, "coordinate":{ "type":"array", "description":"(x, y): 从左/顶边的像素数。对于 action=mouse_move 和 action=left_click 是必需的。" } } } }</tools>
每次调用函数时，返回一个包含函数名称和参数的 JSON 对象，位于 <...> 标签内：
{{"name": <function-name>, "arguments": <args-json-object>}}
此外，如果你认为任务不可行（例如，任务与图像无关），请返回：
{"name": "computer_use", "arguments": {"action": "terminate", "status": "failure"}}
42